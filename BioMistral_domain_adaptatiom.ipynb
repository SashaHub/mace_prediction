{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr3EOWrVYR2S",
        "outputId": "3af5d465-d14d-4551-9a77-cf0dcb99af04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1H6vjwMXPNa"
      },
      "source": [
        "# Imports and installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_wxKmx_XI5T",
        "outputId": "1fade2e9-3f0f-4e97-f038-7dbf86b33c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.14.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m224.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install xmltodict\n",
        "!pip install transformers datasets accelerate peft bitsandbytes sentencepiece\n",
        "!pip install --upgrade --no-cache-dir --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN0T4wALW7Qb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import xmltodict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json # Для отладки и сохранения промежуточных данных, если нужно\n",
        "\n",
        "# --- Константы и определения ---\n",
        "\n",
        "# Коды МКБ-10 для MACE (можно расширить при необходимости)\n",
        "# Инфаркт миокарда (острый и повторный)\n",
        "MI_CODES = [f\"I21.{i}\" for i in range(10)] + \\\n",
        "           [f\"I21.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "           [f\"I22.{i}\" for i in range(10)] + \\\n",
        "           [f\"I22.{i}{j}\" for i in range(10) for j in range(10)]\n",
        "# Инсульты\n",
        "STROKE_CODES = [f\"I60.{i}\" for i in range(10)] + \\\n",
        "               [f\"I60.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I61.{i}\" for i in range(10)] + \\\n",
        "               [f\"I61.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I62.{i}\" for i in range(10)] + \\\n",
        "               [f\"I62.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I63.{i}\" for i in range(10)] + \\\n",
        "               [f\"I63.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I64\"] + \\\n",
        "               [f\"I64.{i}\" for i in range(10)] + \\\n",
        "               [f\"I64.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I65.{i}\" for i in range(10)] + \\\n",
        "               [f\"I65.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I66.{i}\" for i in range(10)] + \\\n",
        "               [f\"I66.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I67.{i}\" for i in range(10)] + \\\n",
        "               [f\"I67.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I68.{i}\" for i in range(10)] + \\\n",
        "               [f\"I68.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I69.{i}\" for i in range(10)] + \\\n",
        "               [f\"I69.{i}{j}\" for i in range(10) for j in range(10)]\n",
        "\n",
        "MACE_ICD_CODES = set(MI_CODES + STROKE_CODES)\n",
        "\n",
        "# Возможные отображения смерти в поле исхода госпитализации (может потребоваться дополнение)\n",
        "DEATH_DISPLAY_NAMES = [\"смерть\", \"умер\", \"умерла\", \"летальный исход\"]\n",
        "\n",
        "# Общий начальный путь для многих секций в структуре CDA\n",
        "# ['ClinicalDocument', 'component', 'structuredBody', 'component', 'section']\n",
        "BASE_PATH_STRUCTURED_BODY_COMP_SECTION = ['ClinicalDocument', 'component', 'structuredBody', 'component']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRUWeqJKXWX-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6n_XrhHXYAa"
      },
      "outputs": [],
      "source": [
        "def find_section_by_path(data_dict, path, default=None):\n",
        "    \"\"\"\n",
        "    Безопасно извлекает значение из вложенного словаря по списку ключей/индексов.\n",
        "    Args:\n",
        "        data_dict (dict): Словарь с данными документа.\n",
        "        path (list): Список ключей и/или индексов для навигации.\n",
        "        default: Значение по умолчанию, если путь не найден.\n",
        "    Returns:\n",
        "        Извлеченное значение или default.\n",
        "    \"\"\"\n",
        "    current = data_dict\n",
        "    try:\n",
        "        for key_or_index in path:\n",
        "            if isinstance(current, list):\n",
        "                current = current[key_or_index]\n",
        "            else:\n",
        "                current = current[key_or_index]\n",
        "        return current\n",
        "    except (KeyError, IndexError, TypeError):\n",
        "        return default\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Очищает текст от лишних символов, множественных пробелов, заменяет плейсхолдеры.\n",
        "    Сохраняет цифры, точки, запятые, дефисы, русские и латинские буквы, знаки процента, скобки, слеши.\n",
        "    Args:\n",
        "        text (str): Входная строка.\n",
        "    Returns:\n",
        "        str: Очищенная строка.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.replace('!!!!!!!!!!!!!', ' ')\n",
        "    text = text.replace('<.>', ' ')\n",
        "    text = re.sub(r'[^\\w\\s\\.,/\\-\\%()\\<\\>]', ' ', text, flags=re.UNICODE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def parse_xml_file(file_path):\n",
        "    \"\"\"\n",
        "    Читает XML файл и конвертирует его в словарь Python.\n",
        "    Args:\n",
        "        file_path (str): Путь к XML файлу.\n",
        "    Returns:\n",
        "        dict: Словарь, представляющий XML, или None при ошибке.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            xml_content = f.read()\n",
        "        xml_content = re.sub(r'<\\?xml-stylesheet.*?\\?>', '', xml_content)\n",
        "        data_dict = xmltodict.parse(xml_content)\n",
        "        return data_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка парсинга XML файла {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_table_text_from_html_like(text_node):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое из HTML-подобной таблицы внутри XML-узла <text>.\n",
        "    Args:\n",
        "        text_node: Узел <text> из XML, преобразованный в словарь.\n",
        "    Returns:\n",
        "        str: Конкатенированный текст из ячеек таблицы.\n",
        "    \"\"\"\n",
        "    if not text_node or not isinstance(text_node, dict):\n",
        "        return \"\"\n",
        "    all_text_parts = []\n",
        "    def recurse_extract(element):\n",
        "        if isinstance(element, str):\n",
        "            cleaned = clean_text(element)\n",
        "            if cleaned: all_text_parts.append(cleaned)\n",
        "        elif isinstance(element, dict):\n",
        "            if '#text' in element:\n",
        "                cleaned = clean_text(element['#text'])\n",
        "                if cleaned: all_text_parts.append(cleaned)\n",
        "            for key in element:\n",
        "                if key != '#text':\n",
        "                    recurse_extract(element[key])\n",
        "        elif isinstance(element, list):\n",
        "            for item in element:\n",
        "                recurse_extract(item)\n",
        "    recurse_extract(text_node)\n",
        "    return \" \".join(all_text_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUCq61pHXa6X"
      },
      "outputs": [],
      "source": [
        "def get_patient_id(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает идентификатор пациента из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> id (первый элемент списка) -> @extension.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: ID пациента или None, если идентификатор не найден по указанному пути.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'id', 0, '@extension'])\n",
        "\n",
        "def get_patient_sex(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает пол пациента из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> patient -> administrativeGenderCode -> @displayName.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Пол пациента (например, \"Мужской\", \"Женский\") или None, если информация о поле не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'patient', 'administrativeGenderCode', '@displayName'])\n",
        "\n",
        "def get_patient_birth_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату рождения пациента в виде строки из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> patient -> birthTime -> @value.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата рождения в формате \"YYYYMMDD...\" или None, если дата рождения не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'patient', 'birthTime', '@value'])\n",
        "\n",
        "def get_admission_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату поступления в стационар в виде строки из структуры документа CDA.\n",
        "    Проверяет два возможных пути, так как дата может находиться в разных секциях:\n",
        "    1. ClinicalDocument -> documentationOf -> serviceEvent -> effectiveTime -> low -> @value\n",
        "    2. ClinicalDocument -> componentOf -> encompassingEncounter -> effectiveTime -> low -> @value\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата поступления в формате \"YYYYMMDD...\" или None, если дата не найдена по обоим путям.\n",
        "    \"\"\"\n",
        "    date_val = find_section_by_path(doc_dict, ['ClinicalDocument', 'documentationOf', 'serviceEvent', 'effectiveTime', 'low', '@value'])\n",
        "    if date_val:\n",
        "        return date_val\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'effectiveTime', 'low', '@value'])\n",
        "\n",
        "def get_discharge_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату выписки из стационара в виде строки из структуры документа CDA.\n",
        "    Проверяет два возможных пути, аналогично дате поступления, но использует 'high' вместо 'low':\n",
        "    1. ClinicalDocument -> documentationOf -> serviceEvent -> effectiveTime -> high -> @value\n",
        "    2. ClinicalDocument -> componentOf -> encompassingEncounter -> effectiveTime -> high -> @value\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата выписки в формате \"YYYYMMDD...\" или None, если дата не найдена по обоим путям.\n",
        "    \"\"\"\n",
        "    date_val = find_section_by_path(doc_dict, ['ClinicalDocument', 'documentationOf', 'serviceEvent', 'effectiveTime', 'high', '@value'])\n",
        "    if date_val:\n",
        "        return date_val\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'effectiveTime', 'high', '@value'])\n",
        "\n",
        "def get_age_at_admission(birth_date_str, admission_date_str):\n",
        "    \"\"\"\n",
        "    Вычисляет возраст пациента в полных годах на момент поступления.\n",
        "    Для вычисления используются только год, месяц и день из предоставленных строк дат.\n",
        "    Args:\n",
        "        birth_date_str (str): Строка с датой рождения в формате \"YYYYMMDD...\" (или None).\n",
        "        admission_date_str (str): Строка с датой поступления в формате \"YYYYMMDD...\" (или None).\n",
        "    Returns:\n",
        "        int: Возраст в полных годах или None, если одна из дат отсутствует или имеет некорректный формат.\n",
        "    \"\"\"\n",
        "    if not birth_date_str or not admission_date_str:\n",
        "        return None\n",
        "    try:\n",
        "        # Парсим только первые 8 символов (YYYYMMDD)\n",
        "        birth_date = datetime.strptime(birth_date_str[:8], \"%Y%m%d\")\n",
        "        admission_date = datetime.strptime(admission_date_str[:8], \"%Y%m%d\")\n",
        "        # Вычисляем возраст: разница лет минус 1, если день рождения в текущем году еще не наступил\n",
        "        age = admission_date.year - birth_date.year - ((admission_date.month, admission_date.day) < (birth_date.month, birth_date.day))\n",
        "        return age\n",
        "    except ValueError:\n",
        "        # Возвращаем None, если формат даты некорректен для парсинга\n",
        "        return None\n",
        "\n",
        "def find_section_by_display_name(doc_dict, base_path_list, target_display_name):\n",
        "    \"\"\"\n",
        "    Находит и извлекает текстовое содержимое секции по ее @displayName в структуре документа CDA.\n",
        "    Рекурсивно ищет на двух уровнях вложенности секций, начиная с указанного базового пути.\n",
        "    Пытается извлечь текст из '#text', напрямую из строки или из HTML-подобной структуры (таблицы).\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "        base_path_list (list): Список ключей/индексов, указывающий базовый путь к компонентам,\n",
        "                               которые могут содержать искомые секции (например, путь к structuredBody/component).\n",
        "        target_display_name (str): Значение атрибута @displayName искомой секции.\n",
        "    Returns:\n",
        "        str: Очищенное текстовое содержимое найденной секции или пустая строка, если секция не найдена\n",
        "             или не содержит извлекаемого текста.\n",
        "    \"\"\"\n",
        "    # Извлекаем основные компоненты, которые могут содержать секции, используя безопасный поиск\n",
        "    main_components = find_section_by_path(doc_dict, base_path_list, [])\n",
        "    # Убеждаемся, что main_components является списком для удобства итерации\n",
        "    if not isinstance(main_components, list):\n",
        "        main_components = [main_components]\n",
        "\n",
        "    # Обходим компоненты первого уровня вложенности\n",
        "    for comp_level1 in main_components:\n",
        "        # Проверяем, что компонент существует и содержит ключ 'section'\n",
        "        if not comp_level1 or 'section' not in comp_level1: continue\n",
        "        section_level1 = comp_level1['section']\n",
        "\n",
        "        sections_to_check_lvl1 = []\n",
        "        # Преобразуем section_level1 в список, если это не список (для случая одной секции)\n",
        "        if isinstance(section_level1, list): sections_to_check_lvl1.extend(section_level1)\n",
        "        elif isinstance(section_level1, dict): sections_to_check_lvl1.append(section_level1)\n",
        "\n",
        "        # Обходим секции первого уровня\n",
        "        for sec_l1_dict in sections_to_check_lvl1:\n",
        "            if not sec_l1_dict: continue\n",
        "            # Проверяем, совпадает ли @displayName текущей секции с целевым\n",
        "            if sec_l1_dict.get('code', {}).get('@displayName') == target_display_name:\n",
        "                # Если нашли секцию, пытаемся извлечь ее текстовое содержимое\n",
        "                text_node = sec_l1_dict.get('text')\n",
        "                # Проверяем разные возможные места хранения текста: '#text', прямой текст, entry/observation/value, HTML-подобная таблица\n",
        "                if isinstance(text_node, dict) and '#text' in text_node: return clean_text(text_node['#text'])\n",
        "                if isinstance(text_node, str): return clean_text(text_node)\n",
        "                entry_val = find_section_by_path(sec_l1_dict, ['entry', 'observation', 'value'])\n",
        "                if isinstance(entry_val, dict) and '#text' in entry_val: return clean_text(entry_val['#text'])\n",
        "                if isinstance(entry_val, str): return clean_text(entry_val)\n",
        "                content_val = find_section_by_path(text_node, ['content'])\n",
        "                if content_val: return extract_table_text_from_html_like(text_node)\n",
        "\n",
        "            # Если секция первого уровня не целевая, проверяем ее вложенные компоненты (второй уровень)\n",
        "            components_level2 = find_section_by_path(sec_l1_dict, ['component'], [])\n",
        "            # Убеждаемся, что components_level2 является списком\n",
        "            if not isinstance(components_level2, list):\n",
        "                components_level2 = [components_level2]\n",
        "\n",
        "            # Обходим компоненты второго уровня\n",
        "            for comp_level2 in components_level2:\n",
        "                # Проверяем, что компонент существует и содержит ключ 'section'\n",
        "                if not comp_level2 or 'section' not in comp_level2: continue\n",
        "                section_level2 = comp_level2['section']\n",
        "                sections_to_check_lvl2 = []\n",
        "                # Преобразуем section_level2 в список\n",
        "                if isinstance(section_level2, list): sections_to_check_lvl2.extend(section_level2)\n",
        "                elif isinstance(section_level2, dict): sections_to_check_lvl2.append(section_level2)\n",
        "\n",
        "                # Обходим секции второго уровня\n",
        "                for sec_l2_dict in sections_to_check_lvl2:\n",
        "                    if not sec_l2_dict: continue\n",
        "                    # Проверяем, совпадает ли @displayName текущей секции с целевым\n",
        "                    if sec_l2_dict.get('code', {}).get('@displayName') == target_display_name:\n",
        "                        # Если нашли секцию, пытаемся извлечь ее текстовое содержимое (аналогично первому уровню)\n",
        "                        text_node = sec_l2_dict.get('text')\n",
        "                        if isinstance(text_node, dict) and '#text' in text_node: return clean_text(text_node['#text'])\n",
        "                        if isinstance(text_node, str): return clean_text(text_node)\n",
        "                        entry_val = find_section_by_path(sec_l2_dict, ['entry', 'observation', 'value'])\n",
        "                        if isinstance(entry_val, dict) and '#text' in entry_val: return clean_text(entry_val['#text'])\n",
        "                        if isinstance(entry_val, str): return clean_text(entry_val)\n",
        "                        content_val = find_section_by_path(text_node, ['content'])\n",
        "                        if content_val: return extract_table_text_from_html_like(text_node)\n",
        "    return \"\" # Возвращаем пустую строку, если секция с целевым @displayName не найдена на обоих уровнях.\n",
        "\n",
        "def get_anamnesis_disease(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Анамнез заболевания\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст анамнеза заболевания или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Анамнез заболевания\")\n",
        "\n",
        "def get_anamnesis_life(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Анамнез жизни\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст анамнеза жизни или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Анамнез жизни\")\n",
        "\n",
        "def get_condition_on_admission(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Состояние при поступлении\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст описания состояния при поступлении или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Состояние при поступлении\")\n",
        "\n",
        "def get_diagnoses_from_section(section_dict, section_title_target):\n",
        "    \"\"\"\n",
        "    Извлекает список диагнозов (код МКБ и текст описания) из заданной секции документа CDA.\n",
        "    Функция ищет вложенные секции с @displayName=\"Диагнозы\" и проверяет их заголовок на соответствие section_title_target.\n",
        "    Диагнозы извлекаются как из структуры <entry>/<observation>, так и из HTML-подобных таблиц в <text>.\n",
        "    Args:\n",
        "        section_dict (dict): Словарь, представляющий родительскую секцию, которая может содержать вложенные секции \"Диагнозы\".\n",
        "        section_title_target (str): Ожидаемое начало текста заголовка вложенной секции \"Диагнозы\"\n",
        "                                    (например, \"Установленные диагнозы при поступлении\").\n",
        "    Returns:\n",
        "        list: Список словарей, каждый из которых представляет диагноз: {'mkb_code': '...', 'text': '...'}.\n",
        "              Возвращает пустой список, если диагнозы не найдены.\n",
        "    \"\"\"\n",
        "    diagnoses = []\n",
        "    # Ищем вложенные компоненты внутри родительской секции\n",
        "    components = find_section_by_path(section_dict, ['component'], [])\n",
        "    # Убеждаемся, что components является списком\n",
        "    if not isinstance(components, list): components = [components]\n",
        "\n",
        "    # Обходим вложенные компоненты\n",
        "    for comp in components:\n",
        "        diag_section = comp.get('section', {})\n",
        "        # Проверяем, является ли вложенная секция секцией диагнозов с нужным @displayName и заголовком\n",
        "        if diag_section.get('code', {}).get('@displayName') == \"Диагнозы\" and \\\n",
        "           clean_text(find_section_by_path(diag_section,['title','#text'],\"\")).startswith(section_title_target):\n",
        "\n",
        "            # --- Извлечение диагнозов из структуры <entry>/<observation> ---\n",
        "            entries = find_section_by_path(diag_section, ['entry'], [])\n",
        "            # Убеждаемся, что entries является списком\n",
        "            if not isinstance(entries, list): entries = [entries]\n",
        "            for entry_item in entries:\n",
        "                # Пропускаем записи с определенным codeSystem (часто служебные или недиагнозы)\n",
        "                if find_section_by_path(entry_item, ['act', 'code', '@codeSystem']) == \"1.2.643.5.1.13.13.99.2.795\":\n",
        "                    continue\n",
        "                # Диагноз может быть вложен в entryRelationship/observation\n",
        "                obs_list = find_section_by_path(entry_item, ['observation', 'entryRelationship', 'observation'], [])\n",
        "                # Убеждаемся, что obs_list является списком\n",
        "                if not isinstance(obs_list, list): obs_list = [obs_list]\n",
        "                for obs in obs_list:\n",
        "                    if not obs: continue\n",
        "                    # Извлекаем код МКБ (@code) и текст диагноза (#text или @displayName)\n",
        "                    icd_code = find_section_by_path(obs, ['value', '@code'])\n",
        "                    diag_text = clean_text(find_section_by_path(obs, ['text', '#text']))\n",
        "                    if not diag_text: # Если текст не найден в <text>, ищем в @displayName value\n",
        "                        diag_text = clean_text(find_section_by_path(obs, ['value', '@displayName']))\n",
        "                    if icd_code and diag_text:\n",
        "                        diagnoses.append({'mkb_code': icd_code.strip(), 'text': diag_text.strip()})\n",
        "\n",
        "            # --- Извлечение диагнозов из HTML-подобной таблицы в <text> ---\n",
        "            text_node = diag_section.get('text')\n",
        "            if text_node and 'table' in text_node:\n",
        "                table_data = text_node['table']\n",
        "                tbody = table_data.get('tbody')\n",
        "                if tbody and 'tr' in tbody:\n",
        "                    rows = tbody['tr']\n",
        "                    # Убеждаемся, что rows является списком строк\n",
        "                    if not isinstance(rows, list): rows = [rows]\n",
        "                    for row in rows:\n",
        "                        cols = row.get('td') # Ячейки таблицы\n",
        "                        # Проверяем, что это список ячеек и их достаточно (номер, описание, код)\n",
        "                        if isinstance(cols, list) and len(cols) >= 3:\n",
        "                            # Извлекаем описание и код из соответствующих ячеек, проверяя разные пути к тексту\n",
        "                            col1_content = find_section_by_path(cols[1], ['content', '#text']) or find_section_by_path(cols[1],['content']) or find_section_by_path(cols[1],['#text'])\n",
        "                            col2_content = find_section_by_path(cols[2], ['content', '#text']) or find_section_by_path(cols[2],['content']) or find_section_by_path(cols[2],['#text'])\n",
        "                            desc_text = clean_text(col1_content)\n",
        "                            code_text = clean_text(col2_content)\n",
        "                            if code_text and desc_text:\n",
        "                                 diagnoses.append({'mkb_code': code_text.strip(), 'text': desc_text.strip()})\n",
        "    return diagnoses\n",
        "\n",
        "def get_all_diagnoses(doc_dict, admission_or_discharge=\"admission\"):\n",
        "    \"\"\"\n",
        "    Извлекает все диагнозы (при поступлении или при выписке) из документа CDA.\n",
        "    Функция ищет диагнозы в различных стандартных секциях (\"Состояние при поступлении\",\n",
        "    \"Пребывание в стационаре\", \"Состояние при выписке\") и их вложенных секциях \"Диагнозы\".\n",
        "    Также включает логику извлечения диагнозов из таблиц в секции \"Пребывание в стационаре\".\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "        admission_or_discharge (str): Указывает, какие диагнозы искать: \"admission\" для диагнозов при поступлении,\n",
        "                                      \"discharge\" для диагнозов при выписке.\n",
        "    Returns:\n",
        "        list: Список уникальных словарей {'mkb_code': '...', 'text': '...'} найденных диагнозов.\n",
        "              Уникальность определяется по коду МКБ (без подкатегории) и тексту диагноза.\n",
        "              Возвращает пустой список, если диагнозы не найдены.\n",
        "    \"\"\"\n",
        "    all_diagnoses_list = []\n",
        "    # Ищем контейнеры секций в структурированном теле документа\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Определяем целевые названия родительских секций и заголовков вложенных секций \"Диагнозы\"\n",
        "    # в зависимости от того, ищем ли мы диагнозы при поступлении или при выписке.\n",
        "    target_parent_section_name = \"Пребывание в стационаре\" if admission_or_discharge == \"admission\" else \"Состояние при выписке\"\n",
        "    target_diag_section_title = \"Установленные диагнозы при поступлении\" if admission_or_discharge == \"admission\" else \"Установленные диагнозы при выписке\"\n",
        "\n",
        "    # Обходим контейнеры секций\n",
        "    for section_container_item in section_containers:\n",
        "        if not section_container_item or 'section' not in section_container_item: continue # Пропускаем пустые или без секций\n",
        "        current_section_candidates = section_container_item['section']\n",
        "        # Убеждаемся, что current_section_candidates является списком\n",
        "        if not isinstance(current_section_candidates, list): current_section_candidates = [current_section_candidates]\n",
        "\n",
        "        # Обходим секции первого уровня в контейнере\n",
        "        for current_section_dict in current_section_candidates:\n",
        "            if not current_section_dict: continue\n",
        "            # Получаем @displayName текущей секции\n",
        "            parent_display_name = current_section_dict.get('code', {}).get('@displayName')\n",
        "\n",
        "            # Если это секция \"Состояние при поступлении\" и мы ищем диагнозы при поступлении\n",
        "            if parent_display_name == \"Состояние при поступлении\" and admission_or_discharge == \"admission\":\n",
        "                 # Извлекаем диагнозы из этой секции, используя целевой заголовок\n",
        "                 diagnoses = get_diagnoses_from_section(current_section_dict, target_diag_section_title)\n",
        "                 all_diagnoses_list.extend(diagnoses)\n",
        "            # Если это целевая родительская секция (\"Пребывание в стационаре\" или \"Состояние при выписке\")\n",
        "            elif parent_display_name == target_parent_section_name:\n",
        "                # Извлекаем диагнозы из этой секции, используя целевой заголовок\n",
        "                diagnoses = get_diagnoses_from_section(current_section_dict, target_diag_section_title)\n",
        "                all_diagnoses_list.extend(diagnoses)\n",
        "            # Специальная логика для извлечения диагнозов из таблицы в секции \"Пребывание в стационаре\"\n",
        "            # (часто там указываются основные диагнозы пребывания)\n",
        "            if admission_or_discharge == \"admission\" and parent_display_name == \"Пребывание в стационаре\":\n",
        "                text_node_main = current_section_dict.get('text')\n",
        "                if text_node_main and 'table' in text_node_main:\n",
        "                    # Проверяем заголовки таблицы, чтобы убедиться, что это таблица диагнозов\n",
        "                    headers_text = extract_table_text_from_html_like(text_node_main.get('table',{}).get('thead',{}))\n",
        "                    if \"Вид нозологической единицы\" in headers_text and \"Код по МКБ-10\" in headers_text:\n",
        "                        table_data = text_node_main['table']\n",
        "                        tbody = table_data.get('tbody')\n",
        "                        if tbody and 'tr' in tbody:\n",
        "                            rows = tbody['tr']\n",
        "                            # Убеждаемся, что rows является списком строк\n",
        "                            if not isinstance(rows, list): rows = [rows]\n",
        "                            for row in rows:\n",
        "                                cols = row.get('td') # Ячейки таблицы\n",
        "                                # Проверяем, что это список ячеек и их достаточно\n",
        "                                if isinstance(cols, list) and len(cols) >= 3:\n",
        "                                    # Извлекаем описание и код из соответствующих ячеек\n",
        "                                    desc_node_val = find_section_by_path(cols[1], ['content', '#text']) or find_section_by_path(cols[1],['content']) or find_section_by_path(cols[1],['#text'])\n",
        "                                    code_node_val = find_section_by_path(cols[2], ['content', '#text']) or find_section_by_path(cols[2],['content']) or find_section_by_path(cols[2],['#text'])\n",
        "                                    desc_text = clean_text(desc_node_val)\n",
        "                                    code_text = clean_text(code_node_val)\n",
        "                                    if code_text and desc_text:\n",
        "                                        all_diagnoses_list.append({'mkb_code': code_text, 'text': desc_text})\n",
        "\n",
        "    # Удаляем дубликаты диагнозов. Уникальность определяется по коду МКБ (берем только основную часть) и тексту.\n",
        "    unique_diagnoses = []\n",
        "    seen_tuples = set()\n",
        "    for d in all_diagnoses_list:\n",
        "        mkb_code_cleaned = d['mkb_code'].split()[0] if d['mkb_code'] else \"\" # Берем только часть кода до первого пробела (например, \"I21.4\" из \"I21.4 MACE\")\n",
        "        if mkb_code_cleaned and (mkb_code_cleaned, d['text']) not in seen_tuples:\n",
        "            unique_diagnoses.append({'mkb_code': mkb_code_cleaned, 'text': d['text']})\n",
        "            seen_tuples.add((mkb_code_cleaned, d['text']))\n",
        "    return unique_diagnoses\n",
        "\n",
        "def get_discharge_outcome(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает исход госпитализации из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> componentOf -> encompassingEncounter -> dischargeDispositionCode -> @displayName.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Описание исхода госпитализации (@displayName, например \"выписан\", \"смерть\")\n",
        "             или None, если исход не найден.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'dischargeDispositionCode', '@displayName'])\n",
        "\n",
        "def get_instrumental_studies_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секций с инструментальными исследованиями из документа CDA.\n",
        "    Функция ищет секции внутри структуры 'PATIENTROUTE' (маршрут пациента) и 'PROC' (процедуры).\n",
        "    Собирает названия отделений, исследований, текст протоколов/результатов (включая таблицы)\n",
        "    и детали исследований из entry/observation.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Объединенный текст из всех найденных секций исследований, очищенный.\n",
        "             Возвращает пустую строку, если секции исследований не найдены.\n",
        "    \"\"\"\n",
        "    studies_texts = []\n",
        "    # Ищем контейнеры секций в структурированном теле документа\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Обходим контейнеры секций\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        top_level_section = sc['section']\n",
        "        sections_to_scan_for_patientroute = []\n",
        "        # Преобразуем top_level_section в список\n",
        "        if isinstance(top_level_section, list): sections_to_scan_for_patientroute.extend(top_level_section)\n",
        "        else: sections_to_scan_for_patientroute.append(top_level_section)\n",
        "\n",
        "        # Ищем секцию с кодом 'PATIENTROUTE' (маршрут пациента)\n",
        "        for s_l1 in sections_to_scan_for_patientroute:\n",
        "            if not s_l1: continue\n",
        "            if find_section_by_path(s_l1, ['code', '@code']) == 'PATIENTROUTE':\n",
        "                # Ищем вложенные компоненты (обычно отделения или этапы маршрута)\n",
        "                departments_components = find_section_by_path(s_l1, ['component'], [])\n",
        "                # Убеждаемся, что departments_components является списком\n",
        "                if not isinstance(departments_components, list): departments_components = [departments_components]\n",
        "                for dept_comp in departments_components:\n",
        "                    dept_section = dept_comp.get('section')\n",
        "                    if not dept_section: continue\n",
        "                    # Извлекаем название отделения/этапа\n",
        "                    dept_name = clean_text(find_section_by_path(dept_section, ['title', '#text']))\n",
        "                    if dept_name: studies_texts.append(f\"Отделение: {dept_name}\")\n",
        "\n",
        "                    # Ищем вложенные компоненты с секциями процедур ('PROC')\n",
        "                    researches_main_component = find_section_by_path(dept_section, ['component'], [])\n",
        "                    # Убеждаемся, что researches_main_component является списком\n",
        "                    if not isinstance(researches_main_component, list): researches_main_component = [researches_main_component]\n",
        "                    for res_main_comp_item in researches_main_component:\n",
        "                        proc_section = res_main_comp_item.get('section')\n",
        "                        # Проверяем, что это секция процедур\n",
        "                        if not proc_section or find_section_by_path(proc_section,['code','@code']) != 'PROC':\n",
        "                            continue\n",
        "                        # Ищем вложенные компоненты с деталями исследований\n",
        "                        actual_researches_components = find_section_by_path(proc_section, ['component'],[])\n",
        "                        # Убеждаемся, что actual_researches_components является списком\n",
        "                        if not isinstance(actual_researches_components, list): actual_researches_components = [actual_researches_components]\n",
        "                        for actual_res_comp in actual_researches_components:\n",
        "                            research_detail_section = actual_res_comp.get('section')\n",
        "                            if not research_detail_section: continue\n",
        "                            # Извлекаем название исследования\n",
        "                            research_name = clean_text(find_section_by_path(research_detail_section, ['title', '#text']))\n",
        "                            if research_name: studies_texts.append(f\"Исследование ({dept_name if dept_name else 'Общее'}): {research_name}\")\n",
        "\n",
        "                            # Извлекаем текст протокола/результатов из <text> (часто в виде таблиц)\n",
        "                            text_node = research_detail_section.get('text')\n",
        "                            table_text = extract_table_text_from_html_like(text_node)\n",
        "                            if table_text: studies_texts.append(f\"Протокол/Результаты: {table_text}\")\n",
        "\n",
        "                            # Извлекаем детали исследования из entry/observation\n",
        "                            entries = find_section_by_path(research_detail_section, ['entry'], [])\n",
        "                            # Убеждаемся, что entries является списком\n",
        "                            if not isinstance(entries, list): entries = [entries]\n",
        "                            for entry in entries:\n",
        "                                # Извлекаем название параметра (@displayName code) и его значение (#text value или @displayName value)\n",
        "                                obs_code_dn = clean_text(find_section_by_path(entry, ['observation', 'code', '@displayName']))\n",
        "                                obs_value_text = clean_text(find_section_by_path(entry, ['observation', 'value', '#text']))\n",
        "                                obs_value_dn = clean_text(find_section_by_path(entry, ['observation', 'value', '@displayName']))\n",
        "                                if obs_code_dn and obs_value_text:\n",
        "                                    studies_texts.append(f\"{obs_code_dn}: {obs_value_text}\")\n",
        "                                elif obs_code_dn and obs_value_dn and obs_value_dn != obs_code_dn : # Добавляем значение, если оно отличается от названия параметра\n",
        "                                    studies_texts.append(f\"{obs_code_dn}: {obs_value_dn}\")\n",
        "                                elif obs_value_text: # Если нет названия параметра, добавляем только текстовое значение\n",
        "                                    studies_texts.append(f\"Деталь исследования: {obs_value_text}\")\n",
        "\n",
        "    # Объединяем все собранные текстовые части исследований в одну строку, удаляя пустые\n",
        "    return \" \".join(filter(None, studies_texts))\n",
        "\n",
        "def get_general_hospitalization_info_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции с общими сведениями о госпитализации ('HOSP').\n",
        "    Ищет секцию по ее коду 'HOSP' на первом уровне вложенности в структурированном теле.\n",
        "    Извлекает текст из HTML-подобной структуры внутри <text>.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст общих сведений о госпитализации или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "    # Обходим контейнеры секций\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        hosp_section_candidates = sc['section']\n",
        "        # Убеждаемся, что hosp_section_candidates является списком\n",
        "        if not isinstance(hosp_section_candidates, list): hosp_section_candidates = [hosp_section_candidates]\n",
        "        # Ищем секцию с кодом 'HOSP'\n",
        "        for hs in hosp_section_candidates:\n",
        "            if hs and find_section_by_path(hs,['code','@code']) == 'HOSP':\n",
        "                 text_node = hs.get('text')\n",
        "                 # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                 return extract_table_text_from_html_like(text_node)\n",
        "    return \"\" # Возвращаем пустую строку, если секция не найдена\n",
        "\n",
        "def get_discharge_condition_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Состояние при выписке\".\n",
        "    Ищет секцию как по @displayName=\"Состояние при выписке\", так и по коду 'STATEDIS'.\n",
        "    По коду 'STATEDIS' ищет на первом уровне вложенности и вложенную в секцию 'HOSP'.\n",
        "    Извлекает текст из HTML-подобной структуры внутри <text>.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст описания состояния при выписке или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    # Сначала пробуем найти по display name (менее надежно, но может сработать)\n",
        "    text_content = find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Состояние при выписке\")\n",
        "\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Обходим контейнеры секций для поиска по коду 'STATEDIS'\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        current_level1_sections = sc['section']\n",
        "        # Убеждаемся, что current_level1_sections является списком\n",
        "        if not isinstance(current_level1_sections, list): current_level1_sections = [current_level1_sections]\n",
        "\n",
        "        # Ищем секцию с кодом 'STATEDIS' на первом уровне\n",
        "        for sec_l1 in current_level1_sections:\n",
        "            if sec_l1 and find_section_by_path(sec_l1,['code','@code']) == 'STATEDIS':\n",
        "                 text_node = sec_l1.get('text')\n",
        "                 # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                 return extract_table_text_from_html_like(text_node)\n",
        "            # Ищем секцию с кодом 'STATEDIS' вложенную в секцию 'HOSP'\n",
        "            if sec_l1 and find_section_by_path(sec_l1,['code','@code']) == 'HOSP':\n",
        "                components_l2 = find_section_by_path(sec_l1, ['component'], [])\n",
        "                # Убеждаемся, что components_l2 является списком\n",
        "                if not isinstance(components_l2, list): components_l2 = [components_l2]\n",
        "                for comp_l2 in components_l2:\n",
        "                    sec_l2 = comp_l2.get('section', {})\n",
        "                    if sec_l2 and find_section_by_path(sec_l2,['code','@code']) == 'STATEDIS':\n",
        "                        text_node = sec_l2.get('text')\n",
        "                        # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                        return extract_table_text_from_html_like(text_node)\n",
        "    # Если по коду не найдено, возвращаем результат поиска по display name (может быть пустой строкой)\n",
        "    return text_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jFubd9cXfN3"
      },
      "outputs": [],
      "source": [
        "def process_patient_records(patient_records_with_meta):\n",
        "    if not patient_records_with_meta:\n",
        "        return None\n",
        "\n",
        "    patient_records_data = [record['data'] for record in patient_records_with_meta]\n",
        "    first_visit_data = patient_records_data[0]\n",
        "\n",
        "    patient_id = get_patient_id(first_visit_data)\n",
        "    sex = get_patient_sex(first_visit_data)\n",
        "    birth_date_str = get_patient_birth_date_str(first_visit_data)\n",
        "    admission_date_str_first = get_admission_date_str(first_visit_data)\n",
        "    age = get_age_at_admission(birth_date_str, admission_date_str_first)\n",
        "\n",
        "    anamnesis_d = get_anamnesis_disease(first_visit_data)\n",
        "    anamnesis_l = get_anamnesis_life(first_visit_data)\n",
        "    condition_adm = get_condition_on_admission(first_visit_data)\n",
        "\n",
        "    diagnoses_adm_list_first_visit = get_all_diagnoses(first_visit_data, \"admission\")\n",
        "    diagnoses_dis_list_first_visit = get_all_diagnoses(first_visit_data, \"discharge\")\n",
        "\n",
        "    diagnoses_adm_texts = [f\"{d['mkb_code']}: {d['text']}\" for d in diagnoses_adm_list_first_visit]\n",
        "    diagnoses_dis_texts_fv = [f\"{d['mkb_code']}: {d['text']}\" for d in diagnoses_dis_list_first_visit]\n",
        "\n",
        "    instrumental_text = get_instrumental_studies_text(first_visit_data)\n",
        "    general_hosp_text = get_general_hospitalization_info_text(first_visit_data)\n",
        "    discharge_condition_text_fv = get_discharge_condition_text(first_visit_data)\n",
        "\n",
        "    text_features_parts = [\n",
        "        f\"Анамнез заболевания: {anamnesis_d}\" if anamnesis_d else \"\",\n",
        "        f\"Анамнез жизни: {anamnesis_l}\" if anamnesis_l else \"\",\n",
        "        f\"Состояние при поступлении: {condition_adm}\" if condition_adm else \"\",\n",
        "        f\"Диагнозы при поступлении: {'; '.join(diagnoses_adm_texts)}\" if diagnoses_adm_texts else \"\",\n",
        "        f\"Диагнозы при выписке (первый визит): {'; '.join(diagnoses_dis_texts_fv)}\" if diagnoses_dis_texts_fv else \"\",\n",
        "        f\"Результаты исследований (первый визит): {instrumental_text}\" if instrumental_text else \"\",\n",
        "        f\"Общие сведения о госпитализации (первый визит): {general_hosp_text}\" if general_hosp_text else \"\",\n",
        "        f\"Состояние при выписке (первый визит): {discharge_condition_text_fv}\" if discharge_condition_text_fv else \"\"\n",
        "    ]\n",
        "    aggregated_text = clean_text(\" \".join(filter(None, text_features_parts)))\n",
        "\n",
        "    # Собираем МКБ коды только со второго и последующих визитов\n",
        "    subsequent_visits_mkb_codes = set()\n",
        "    mace_target = None # Инициализируем таргет\n",
        "\n",
        "    if len(patient_records_data) > 1:\n",
        "        mace_target = 0 # По умолчанию таргет 0, если есть последующие визиты, но нет MACE/смерти\n",
        "        for subsequent_visit_idx in range(1, len(patient_records_data)):\n",
        "            subsequent_visit_data = patient_records_data[subsequent_visit_idx]\n",
        "            discharge_outcome = get_discharge_outcome(subsequent_visit_data)\n",
        "\n",
        "            if discharge_outcome and clean_text(discharge_outcome).lower() in DEATH_DISPLAY_NAMES:\n",
        "                mace_target = 1\n",
        "                break # Найден летальный исход в последующем визите, устанавливаем таргет 1 и выходим\n",
        "\n",
        "            subsequent_diagnoses_adm = get_all_diagnoses(subsequent_visit_data, \"admission\")\n",
        "            subsequent_diagnoses_dis = get_all_diagnoses(subsequent_visit_data, \"discharge\")\n",
        "\n",
        "            # Собираем МКБ коды при поступлении и выписке из последующих визитов\n",
        "            for diag in subsequent_diagnoses_adm + subsequent_diagnoses_dis:\n",
        "                 mkb_code = diag.get('mkb_code', '').strip().upper()\n",
        "                 if mkb_code:\n",
        "                     subsequent_visits_mkb_codes.add(mkb_code)\n",
        "\n",
        "            # Проверяем коды МКБ из последующих визитов на наличие MACE для установки таргета\n",
        "            for diag in subsequent_diagnoses_adm + subsequent_diagnoses_dis:\n",
        "                mkb_code = diag.get('mkb_code', '').strip().upper()\n",
        "                if mkb_code in MACE_ICD_CODES:\n",
        "                    mace_target = 1\n",
        "                    break # Найден MACE код во последующем визите, устанавливаем таргет и выходим\n",
        "\n",
        "            if mace_target == 1: # Если таргет стал 1 в этом визите, завершаем проверку последующих визитов\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        'patient_id': patient_id,\n",
        "        'text_features': aggregated_text,\n",
        "        'age_at_first_admission': age,\n",
        "        'sex': sex,\n",
        "        'mace_target': mace_target,\n",
        "        'unique_mkb_codes': list(subsequent_visits_mkb_codes) # Коды из 2+ визитов\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWweX0UpXkgb"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLswtna4YvAN",
        "outputId": "d0c66673-82b6-41ac-cd42-2a28216ae740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Файл /content/drive/My Drive/med_records_8500.zip успешно распакован в папку /medical_records\n"
          ]
        }
      ],
      "source": [
        "zip_path = '/content/drive/My Drive/med_records_8500.zip'\n",
        "extract_path = '/medical_records'\n",
        "\n",
        "# Создаем папку для извлечения, если она не существует\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Файл {zip_path} успешно распакован в папку {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "kaKFUZcjXkQV",
        "outputId": "26a6de04-3d04-4a31-870b-0c502e8a7f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Найдено 8518 XML файлов. Начинаю обработку...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-581106de56c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxml_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdoc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_xml_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c00f367235cb>\u001b[0m in \u001b[0;36mparse_xml_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mxml_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mxml_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<\\?xml-stylesheet.*?\\?>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, process_comments, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mCharacterData\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mcharacters\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "xml_pattern = \"/medical_records/med_records_8500/*.xml\" # Пример для файлов, начинающихся с EMD_EPIC_DISCHARGE_\n",
        "\n",
        "# Если файлы в другой директории, укажите путь:\n",
        "# current_directory = os.getcwd() # Получаем текущую рабочую директорию ноутбука\n",
        "# xml_pattern = os.path.join(current_directory, \"ваша_папка_с_xml\", \"*.xml\")\n",
        "\n",
        "\n",
        "all_records_by_patient = {}\n",
        "xml_files = glob.glob(xml_pattern)\n",
        "\n",
        "if not xml_files:\n",
        "    print(f\"Не найдено XML файлов по паттерну: {xml_pattern}\")\n",
        "    # Можно остановить выполнение, если файлы не найдены, или продолжить с пустым списком\n",
        "    # raise FileNotFoundError(f\"Не найдено XML файлов по паттерну: {xml_pattern}\")\n",
        "else:\n",
        "    print(f\"Найдено {len(xml_files)} XML файлов. Начинаю обработку...\")\n",
        "\n",
        "    for file_path in xml_files:\n",
        "        doc_dict = parse_xml_file(file_path)\n",
        "        if not doc_dict:\n",
        "            continue\n",
        "\n",
        "        patient_id = get_patient_id(doc_dict)\n",
        "        admission_date_str = get_admission_date_str(doc_dict)\n",
        "\n",
        "        if not patient_id:\n",
        "            print(f\"Не удалось извлечь ID пациента из файла: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        admission_date = None\n",
        "        if admission_date_str:\n",
        "            try:\n",
        "                admission_date = datetime.strptime(admission_date_str[:8], \"%Y%m%d\")\n",
        "            except ValueError:\n",
        "                print(f\"Некорректный формат даты поступления '{admission_date_str}' в файле: {file_path}.\")\n",
        "                admission_date = datetime.max # Для некорректных дат ставим максимальную, чтобы они были в конце\n",
        "        else:\n",
        "             print(f\"Отсутствует дата поступления в файле: {file_path}.\")\n",
        "             admission_date = datetime.max\n",
        "\n",
        "\n",
        "        if patient_id not in all_records_by_patient:\n",
        "            all_records_by_patient[patient_id] = []\n",
        "\n",
        "        all_records_by_patient[patient_id].append({\n",
        "            'admission_date': admission_date,\n",
        "            'data': doc_dict,\n",
        "            'file_path': file_path\n",
        "        })\n",
        "\n",
        "    for patient_id in all_records_by_patient:\n",
        "        all_records_by_patient[patient_id].sort(key=lambda x: x['admission_date'])\n",
        "\n",
        "    print(f\"Сгруппировано {len(all_records_by_patient)} уникальных пациентов.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqpNKM_jXyDW"
      },
      "outputs": [],
      "source": [
        "final_data_list = []\n",
        "processed_patients = 0\n",
        "total_patients = len(all_records_by_patient)\n",
        "\n",
        "if total_patients > 0:\n",
        "    print(f\"Начинаю детальную обработку {total_patients} пациентов...\")\n",
        "    for patient_id, records_with_meta in all_records_by_patient.items():\n",
        "        processed_info = process_patient_records(records_with_meta)\n",
        "        if processed_info:\n",
        "            # Добавляем количество визитов в processed_info\n",
        "            processed_info['num_visits'] = len(records_with_meta)\n",
        "            final_data_list.append(processed_info)\n",
        "        processed_patients +=1\n",
        "        if processed_patients % 100 == 0 or processed_patients == total_patients:\n",
        "            print(f\"Обработано пациентов: {processed_patients}/{total_patients}\")\n",
        "\n",
        "    df_results = pd.DataFrame(final_data_list)\n",
        "\n",
        "    print(\"\\nОбновление таргета MACE на основе кодов выписки из 2+ визитов...\")\n",
        "    initial_mace_1_count = df_results['mace_target'].value_counts().get(1.0, 0)\n",
        "\n",
        "    # Проходим по DataFrame и обновляем таргет\n",
        "    for index, row in df_results.iterrows():\n",
        "        # Обновляем только если текущий таргет не 1 (т.е. не было летального исхода)\n",
        "        # и у пациента было более одного визита\n",
        "        if row['mace_target'] != 1.0 and row['num_visits'] > 1 and row['unique_mkb_codes']:\n",
        "            # Проверяем наличие MACE кодов в unique_mkb_codes (коды выписки 2+ визитов)\n",
        "            if any(code in MACE_ICD_CODES for code in row['unique_mkb_codes']):\n",
        "                df_results.loc[index, 'mace_target'] = 1.0\n",
        "\n",
        "    updated_mace_1_count = df_results['mace_target'].value_counts().get(1.0, 0)\n",
        "    print(f\"Количество пациентов с mace_target=1.0 после обновления: {updated_mace_1_count} (До обновления: {initial_mace_1_count})\")\n",
        "\n",
        "\n",
        "    print(\"\\nИтоговая таблица (первые 5 строк):\")\n",
        "    display(df_results.head())\n",
        "    print(f\"\\nРазмер таблицы: {df_results.shape}\")\n",
        "\n",
        "    if not df_results.empty:\n",
        "        print(f\"\\nРаспределение MACE таргета:\\n{df_results['mace_target'].value_counts(dropna=False)}\")\n",
        "        print(f\"\\nРаспределение количества визитов:\\n{df_results['num_visits'].value_counts().sort_index()}\")\n",
        "    else:\n",
        "        print(\"\\nИтоговая таблица пуста после обработки.\")\n",
        "else:\n",
        "    print(\"Нет данных для обработки (список пациентов пуст).\")\n",
        "    df_results = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5127b92"
      },
      "outputs": [],
      "source": [
        "# Сбор всех уникальных МКБ кодов из DataFrame\n",
        "all_unique_mkb_codes_in_data = set()\n",
        "for index, row in df_results.iterrows():\n",
        "    if row['unique_mkb_codes']:\n",
        "        all_unique_mkb_codes_in_data.update(row['unique_mkb_codes'])\n",
        "\n",
        "print(f\"Общее количество уникальных МКБ кодов в данных: {len(all_unique_mkb_codes_in_data)}\")\n",
        "\n",
        "# Сравнение с MACE_ICD_CODES\n",
        "mace_codes_in_data = all_unique_mkb_codes_in_data.intersection(MACE_ICD_CODES)\n",
        "print(f\"Количество МКБ кодов из списка MACE, найденных в данных: {len(mace_codes_in_data)}\")\n",
        "\n",
        "# Вывод МКБ кодов из списка MACE, которые есть в данных\n",
        "if mace_codes_in_data:\n",
        "    print(\"\\nМКБ коды из списка MACE, найденные в данных:\")\n",
        "    print(sorted(list(mace_codes_in_data)))\n",
        "else:\n",
        "    print(\"\\nВ данных не найдено МКБ кодов, соответствующих списку MACE_ICD_CODES.\")\n",
        "\n",
        "# Вывод МКБ кодов в данных, которые не входят в список MACE_ICD_CODES (первые 20 для примера)\n",
        "other_codes_in_data = list(all_unique_mkb_codes_in_data - MACE_ICD_CODES)\n",
        "print(f\"\\nКоличество других МКБ кодов в данных (не из списка MACE): {len(other_codes_in_data)}\")\n",
        "if other_codes_in_data:\n",
        "    print(\"Примеры других МКБ кодов в данных (первые 20):\")\n",
        "    print(sorted(other_codes_in_data)[:20])\n",
        "\n",
        "# Анализ пациентов с mace_target=0 и NaN\n",
        "mace_0_patients = df_results[df_results['mace_target'] == 0.0]\n",
        "mace_nan_patients = df_results[df_results['mace_target'].isna()]\n",
        "\n",
        "print(f\"\\nКоличество пациентов с mace_target = 0.0: {len(mace_0_patients)}\")\n",
        "print(f\"Количество пациентов с mace_target = NaN: {len(mace_nan_patients)}\")\n",
        "\n",
        "# Проверка наличия MACE кодов у пациентов с mace_target = 0.0\n",
        "mace_0_with_mace_codes = mace_0_patients[mace_0_patients['unique_mkb_codes'].apply(lambda codes: any(code in MACE_ICD_CODES for code in codes))]\n",
        "print(f\"Количество пациентов с mace_target = 0.0, у которых есть МКБ коды из списка MACE: {len(mace_0_with_mace_codes)}\")\n",
        "\n",
        "# Проверка наличия MACE кодов у пациентов с mace_target = NaN\n",
        "mace_nan_with_mace_codes = mace_nan_patients[mace_nan_patients['unique_mkb_codes'].apply(lambda codes: any(code in MACE_ICD_CODES for code in codes))]\n",
        "print(f\"Количество пациентов с mace_target = NaN, у которых есть МКБ коды из списка MACE: {len(mace_nan_with_mace_codes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HpBOJZwX01c"
      },
      "source": [
        "## Save Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUrPNOhrX0g_"
      },
      "outputs": [],
      "source": [
        "# if not df_results.empty:\n",
        "#     output_filename = \"/content/drive/My Drive/parsed_medical_data.parquet\" # Указываем путь на Google Диск и формат Parquet\n",
        "#     df_results.to_parquet(output_filename, index=False) # Используем to_parquet\n",
        "#     print(f\"\\nТаблица сохранена в {output_filename}\")\n",
        "# else:\n",
        "#     print(\"\\nDataFrame пуст, сохранение не выполнено.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MliBRI1gH9M"
      },
      "source": [
        "# LLMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq9BV88xgPeY"
      },
      "source": [
        "## Download again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79593a04"
      },
      "outputs": [],
      "source": [
        "# Загрузка таблицы из Google Диска\n",
        "output_filename = \"/content/drive/My Drive/parsed_medical_data.parquet\"\n",
        "try:\n",
        "    df_results = pd.read_parquet(output_filename)\n",
        "    print(f\"Таблица успешно загружена из {output_filename}\")\n",
        "    display(df_results.head())\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке таблицы из {output_filename}: {e}\")\n",
        "    df_results = pd.DataFrame() # Создаем пустой DataFrame в случае ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f9cc69"
      },
      "source": [
        "## Text data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0432588"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def further_clean_text(text):\n",
        "    \"\"\"\n",
        "    Дополнительная очистка текста. Сохраняет заглавные буквы в начале предложений, аббревиатуры из 2-3 заглавных букв,\n",
        "    остальной текст приводит к нижнему регистру. Удаляет повторяющиеся символы и специфические паттерны.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Список служебных слов/местоимений, которые часто пишутся заглавными по ошибке\n",
        "    PRONOUNS_OR_FUNCTION_WORDS = {\"ЖЕ\", \"ТО\", \"ЛИ\", \"ИЛИ\", \"НЕ\", \"НИ\", \"БЫ\"}\n",
        "\n",
        "    # Предварительная очистка от специфических паттернов и лишних символов\n",
        "    text = text.replace('!!!!!!!!!!!!!', ' ')\n",
        "    text = text.replace('<.>', ' ')\n",
        "    # Заменяем группы небуквенных символов (кроме разрешенных) на пробелы\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s\\.,/\\-\\%()<>]+', ' ', text, flags=re.UNICODE)\n",
        "    # Заменяем множественные пробелы на один и удаляем пробелы по краям\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Обработка заглавных букв и аббревиатур\n",
        "    # Используем более надежное разбиение на предложения, сохраняя разделители для последующего объединения\n",
        "    sentence_parts = re.split(r'([.!?]\\s+)', text)\n",
        "    processed_sentences = []\n",
        "\n",
        "    current_sentence_words = []\n",
        "    is_first_word_of_sentence = True\n",
        "\n",
        "    for part in sentence_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if re.fullmatch(r'[.!?]\\s+', part): # Если часть - это разделитель предложения\n",
        "            if current_sentence_words:\n",
        "                processed_sentences.append(\" \".join(current_sentence_words))\n",
        "            processed_sentences.append(part.strip()) # Добавляем разделитель\n",
        "            current_sentence_words = [] # Начинаем новое предложение\n",
        "            is_first_word_of_sentence = True\n",
        "        else: # Если часть - это текст предложения\n",
        "            words = part.split()\n",
        "            for word in words:\n",
        "                # Проверяем, является ли слово аббревиатурой (2 или 3 заглавные буквы)\n",
        "                if re.fullmatch(r'[А-ЯA-Z]{2,3}', word):\n",
        "                    current_sentence_words.append(word) # Сохраняем как аббревиатуру\n",
        "                    is_first_word_of_sentence = False\n",
        "                # Проверяем, является ли слово из списка служебных слов, написанных заглавными\n",
        "                elif word.upper() in PRONOUNS_OR_FUNCTION_WORDS and word.isupper():\n",
        "                     current_sentence_words.append(word.lower()) # Приводим к нижнему регистру\n",
        "                     is_first_word_of_sentence = False\n",
        "                elif is_first_word_of_sentence and word and word[0].isupper():\n",
        "                     # Сохраняем заглавную букву в начале первого слова предложения\n",
        "                     current_sentence_words.append(word[0] + word[1:].lower())\n",
        "                     is_first_word_of_sentence = False\n",
        "                else:\n",
        "                    current_sentence_words.append(word.lower()) # Приводим остальные слова к нижнему регистру\n",
        "    # Добавляем последнее предложение, если оно не было завершено знаком препинания\n",
        "    if current_sentence_words:\n",
        "         processed_sentences.append(\" \".join(current_sentence_words))\n",
        "\n",
        "    text = \" \".join(processed_sentences).strip() # Объединяем части обратно\n",
        "\n",
        "    # Удаление повторяющихся подряд слов (e.g., \"при при поступлении\")\n",
        "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
        "\n",
        "    # Ищем последовательность из слов, за которой следует та же последовательность\n",
        "    match_repeats = True\n",
        "    while match_repeats:\n",
        "        pattern = r'\\b((?:\\w+\\s+){1,}\\w+)\\s+\\1\\b' # Ищем повторение\n",
        "        new_text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE) # Игнорируем регистр при поиске повторов\n",
        "        if new_text == text:\n",
        "            match_repeats = False # Повторов больше нет\n",
        "        text = new_text\n",
        "\n",
        "\n",
        "\n",
        "    # финальная очистка\n",
        "    text = re.sub(r'(.)(?!\\1)(?<![\\.,])\\1+', r'\\1', text)\n",
        "\n",
        "    # Удаление множественных пробелов\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Применение очистки к столбцу text_features\n",
        "if not df_results.empty:\n",
        "    print(\"Применяю дополнительную очистку к столбцу 'text_features'...\")\n",
        "    df_results['text_features'] = df_results['text_features'].apply(further_clean_text)\n",
        "    print(\"Очистка завершена. Первые 5 строк с новым столбцом 'text_features':\")\n",
        "    display(df_results[['text_features']].head())\n",
        "else:\n",
        "    print(\"DataFrame пуст, дополнительная очистка не выполнена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lAcWJUvftQZ"
      },
      "source": [
        "## BioMistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJuanldftQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40bbd0e2-f73e-47ef-b8fd-327b949f5f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Доступно 1 GPU.\n",
            "Текущее GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Доступно {torch.cuda.device_count()} GPU.\")\n",
        "    print(f\"Текущее GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU недоступно, используется CPU.\")\n",
        "\n",
        "# Для воспроизводимости\n",
        "SEED = 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICe-2NX9ftQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4a4d0d-1165-438c-a08e-027d142c1f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер датасета для обучения и теста: 994\n"
          ]
        }
      ],
      "source": [
        "# Отбираем только данные с известными таргетами\n",
        "df_labeled = df_results[df_results['mace_target'].notna()].copy()\n",
        "df_labeled['mace_target'] = df_labeled['mace_target'].astype(int)\n",
        "print(f\"\\nРазмер датасета для обучения и теста: {len(df_labeled)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7lAGpgKftQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4ec5d4-4de1-4ae2-b383-ea7271dd2da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at BioMistral/BioMistral-7B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Токенизатор загружен. Pad token: </s>, ID: 2\n",
            "Модель BioMistral-7B загружена для Sequence Classification с 2 классами.\n",
            "Конфигурация модели (первые несколько параметров):\n",
            "  vocab_size: 32000\n",
            "  max_position_embeddings: 32768\n",
            "  hidden_size: 4096\n",
            "  intermediate_size: 14336\n",
            "  num_hidden_layers: 32\n",
            "  num_attention_heads: 32\n",
            "  sliding_window: 4096\n",
            "  head_dim: None\n",
            "  num_key_value_heads: 8\n",
            "  hidden_act: silu\n",
            "  initializer_range: 0.02\n",
            "  rms_norm_eps: 1e-05\n",
            "  use_cache: False\n",
            "  rope_theta: 10000.0\n",
            "  attention_dropout: 0.0\n",
            "  return_dict: True\n",
            "  output_hidden_states: False\n",
            "  output_attentions: False\n",
            "  torchscript: False\n",
            "  torch_dtype: float16\n",
            "  use_bfloat16: False\n",
            "  tf_legacy_loss: False\n",
            "  pruned_heads: {}\n",
            "  tie_word_embeddings: False\n",
            "  chunk_size_feed_forward: 0\n",
            "  is_encoder_decoder: False\n",
            "  is_decoder: False\n",
            "  cross_attention_hidden_size: None\n",
            "  add_cross_attention: False\n",
            "  tie_encoder_decoder: False\n",
            "  max_length: 20\n",
            "  min_length: 0\n",
            "  do_sample: False\n",
            "  early_stopping: False\n",
            "  num_beams: 1\n",
            "  num_beam_groups: 1\n",
            "  diversity_penalty: 0.0\n",
            "  temperature: 1.0\n",
            "  top_k: 50\n",
            "  top_p: 1.0\n",
            "  typical_p: 1.0\n",
            "  repetition_penalty: 1.0\n",
            "  length_penalty: 1.0\n",
            "  no_repeat_ngram_size: 0\n",
            "  encoder_no_repeat_ngram_size: 0\n",
            "  bad_words_ids: None\n",
            "  num_return_sequences: 1\n",
            "  output_scores: False\n",
            "  return_dict_in_generate: False\n",
            "  forced_bos_token_id: None\n",
            "  forced_eos_token_id: None\n",
            "  remove_invalid_values: False\n",
            "  exponential_decay_length_penalty: None\n",
            "  suppress_tokens: None\n",
            "  begin_suppress_tokens: None\n",
            "  architectures: ['MistralForCausalLM']\n",
            "  finetuning_task: None\n",
            "  id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
            "  label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
            "  tokenizer_class: None\n",
            "  prefix: None\n",
            "  bos_token_id: 1\n",
            "  pad_token_id: 2\n",
            "  eos_token_id: 2\n",
            "  sep_token_id: None\n",
            "  decoder_start_token_id: None\n",
            "  task_specific_params: None\n",
            "  problem_type: None\n",
            "  _name_or_path: BioMistral/BioMistral-7B\n",
            "  transformers_version: 4.52.2\n",
            "  model_type: mistral\n",
            "  quantization_config: {'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>, '_load_in_8bit': False, '_load_in_4bit': False, 'llm_int8_threshold': 6.0, 'llm_int8_skip_modules': None, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'load_in_4bit': False, 'load_in_8bit': False}\n"
          ]
        }
      ],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "MODEL_NAME = \"BioMistral/BioMistral-7B\"\n",
        "\n",
        "# Квантизация для уменьшения использования памяти\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Для A100 используем bfloat16\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "MODEL_MAX_SEQ_LENGTH = 2048\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    model_max_length=MODEL_MAX_SEQ_LENGTH,\n",
        "    padding_side=\"right\",  # Явно задаем сторону паддинга, \"right\" - частый выбор\n",
        "    truncation_side=\"right\" # Явно задаем сторону обрезки\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2, # Бинарная классификация (MACE есть / MACE нет)\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Настройка padding token если он не установлен\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "print(f\"\\nТокенизатор загружен. Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
        "print(f\"Модель BioMistral-7B загружена для Sequence Classification с {model.config.num_labels} классами.\")\n",
        "print(f\"Конфигурация модели (первые несколько параметров):\")\n",
        "for k, v in list(model.config.to_dict().items()):\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox613SEdftQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015a9a37-fd8c-4eb4-8316-5bb4ce530398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Максимальная длина последовательности для токенизатора (tokenizer.model_max_length): 2048\n",
            "Практическая максимальная длина для токенизации: 2048\n"
          ]
        }
      ],
      "source": [
        "# Проверим model_max_length токенизатора\n",
        "CONTEXT_WINDOW_SIZE = tokenizer.model_max_length\n",
        "print(f\"\\nМаксимальная длина последовательности для токенизатора (tokenizer.model_max_length): {CONTEXT_WINDOW_SIZE}\")\n",
        "PRACTICAL_MAX_LENGTH = MODEL_MAX_SEQ_LENGTH\n",
        "print(f\"Практическая максимальная длина для токенизации: {PRACTICAL_MAX_LENGTH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Run-BdhDftQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87321123-6251-4ee9-f9d6-29906464ea11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3137 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анализ длин токенизированных текстов (на 994 примерах):\n",
            "  Максимальная наблюдаемая длина: 6944 токенов\n",
            "  Средняя наблюдаемая длина: 1654.41 токенов\n",
            "  Медианная наблюдаемая длина: 1320.00 токенов\n",
            "  95-й перцентиль длины: 3392.50 токенов\n",
            "\n",
            " Максимальная наблюдаемая длина (6944) превышает выбранную практическую максимальную длину (2048).\n",
            "   Примерно 306 из 994 (30.78%) текстов будут проанализированны со скользящим окном.\n"
          ]
        }
      ],
      "source": [
        "token_lengths = []\n",
        "for text in df_labeled['text_features']:\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    token_lengths.append(len(tokens))\n",
        "\n",
        "max_observed_len = np.max(token_lengths)\n",
        "avg_observed_len = np.mean(token_lengths)\n",
        "median_observed_len = np.median(token_lengths)\n",
        "percentile_95_len = np.percentile(token_lengths, 95)\n",
        "\n",
        "print(f\"Анализ длин токенизированных текстов (на {len(df_labeled)} примерах):\")\n",
        "print(f\"  Максимальная наблюдаемая длина: {max_observed_len} токенов\")\n",
        "print(f\"  Средняя наблюдаемая длина: {avg_observed_len:.2f} токенов\")\n",
        "print(f\"  Медианная наблюдаемая длина: {median_observed_len:.2f} токенов\")\n",
        "print(f\"  95-й перцентиль длины: {percentile_95_len:.2f} токенов\")\n",
        "\n",
        "if max_observed_len > PRACTICAL_MAX_LENGTH:\n",
        "    print(f\"\\n Максимальная наблюдаемая длина ({max_observed_len}) превышает выбранную практическую максимальную длину ({PRACTICAL_MAX_LENGTH}).\")\n",
        "    num_truncated = sum(1 for length in token_lengths if length > PRACTICAL_MAX_LENGTH)\n",
        "    print(f\"   Примерно {num_truncated} из {len(df_labeled)} ({num_truncated/len(df_labeled)*100:.2f}%) текстов будут проанализированны со скользящим окном.\")\n",
        "else:\n",
        "    print(f\"\\n Все тексты ({max_observed_len} токенов макс.) помещаются в выбранную практическую максимальную длину ({PRACTICAL_MAX_LENGTH}).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIK5eFZAftQa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "b5b79ed21b974df7bb4cfd43be945ada",
            "f319f8c6dbcd49199d737f0e53b456e1",
            "de119047331449ba9abf842563d945e4",
            "d9166923e70747998b2172d57bc5cb87",
            "c0300fcd3a244b93a51be7feb3059178",
            "700d5b454c264bb0bcc376aeaa47b239",
            "c87db75db0554ab78a56d3ffc09ad9b3",
            "725fd1d98e874e9eab84060f77d72fed",
            "162216e18bfc4c3ab0ec10d9b584343c",
            "08f64c9b74e64787b941332033cb6027",
            "1dd2b49b0d3a4452b5aa50c67b219dd8",
            "f58455778013482da633788282279ddf",
            "946e0e9832bd446695e22af24ccaad80",
            "9bd67f565e214c0c8f9df941e717d6c1",
            "04db04f085a74b9a86471e22f1ca44df",
            "2183a8680da549fea8ce70dec241ed17",
            "96f1a20ed077428ab06b623222b4f583",
            "22ceab0eec8540149dc39239b805ed2c",
            "3c6393cbb2fa4916a37fd54b5ab3d434",
            "9bc8ac761f4c490cb0829dbfa75cd33f",
            "04fd078d1de5464f97e75a4332fcdc40",
            "fc981ed0579e46148153b950be5a8bac"
          ]
        },
        "outputId": "999a8ed6-e035-44d0-9501-ff78be4753b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер обучающей выборки: 795\n",
            "Размер тестовой выборки: 199\n",
            "Распределение классов в обучающей выборке: mace_target\n",
            "1    0.579874\n",
            "0    0.420126\n",
            "Name: proportion, dtype: float64\n",
            "Распределение классов в тестовой выборке: mace_target\n",
            "1    0.577889\n",
            "0    0.422111\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Структура датасета Hugging Face:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 795\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 199\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b79ed21b974df7bb4cfd43be945ada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f58455778013482da633788282279ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример токенизированного датасета:\n",
            "{'label': 1, 'input_ids': [1, 2182, 612, 28803, 1185, 28812, 1586, 2290, 1125, 22588, 1878, 922, 669, 2201, 3615, 28861, 28844, 2101, 21083, 27538, 5394, 2734, 842, 8736, 1049, 7490, 2310, 28817, 28811, 698, 27749, 2734, 698, 6048, 1696, 665, 28705, 28734, 28787, 28723, 28734, 28787, 28723, 28750, 28750, 28810, 28725, 28164, 649, 22202, 6515, 8456, 3020, 21697, 1051, 702, 28811, 11084, 28794, 2433, 2084, 1956, 2289, 5005, 8281, 4522, 28773, 5536, 922, 3940, 28858, 9891, 1454, 3596, 28725, 942, 922, 4446, 9468, 28842, 2375, 28874, 28705, 28740, 28734, 6589, 27452, 28725, 9997, 2749, 7921, 13279, 12510, 8378, 9912, 1175, 17921, 842, 3940, 28858, 1894, 28907, 28868, 842, 3787, 2200, 28853, 16283, 2197, 28907, 2802, 28785, 2789, 6770, 7659, 842, 9392, 1788, 1454, 28791, 23706, 3171, 703, 28812, 3596, 5536, 922, 1531, 28860, 28892, 1351, 2084, 9273, 3596, 4111, 28859, 649, 28705, 28740, 28750, 28705, 28770, 28734, 28705, 28734, 28783, 28723, 28734, 28787, 28723, 28750, 28750, 28810, 28725, 942, 922, 4446, 3596, 842, 11824, 1971, 28794, 12510, 10185, 21195, 15977, 28812, 28705, 28740, 28786, 28725, 28705, 2954, 728, 28846, 939, 28705, 28740, 28786, 28725, 2216, 28907, 28896, 1351, 2932, 28826, 25912, 842, 9177, 2200, 10185, 21195, 11061, 1224, 7659, 649, 28705, 1941, 28778, 6826, 20717, 1911, 28817, 28725, 1531, 28874, 8411, 28852, 28872, 387, 5652, 28817, 28888, 2749, 22688, 1454, 8067, 6075, 839, 3647, 24684, 1531, 28874, 27844, 12756, 4887, 939, 4026, 3218, 28942, 842, 12119, 14398, 612, 2775, 28885, 28875, 12382, 728, 1000, 3965, 28819, 842, 1531, 28874, 2573, 12929, 2688, 870, 12301, 4978, 1692, 6775, 649, 2807, 28852, 28868, 5294, 1000, 1563, 1406, 28856, 28847, 842, 2182, 612, 28803, 1185, 28812, 7157, 22037, 1279, 22202, 2676, 2683, 1051, 15796, 1125, 6515, 22202, 2676, 1351, 10410, 1125, 10111, 892, 4446, 8848, 28705, 4483, 1120, 10092, 2197, 28858, 4025, 18664, 6171, 2200, 9745, 2197, 28907, 6978, 28870, 28811, 10646, 2734, 842, 15498, 3171, 703, 28812, 4086, 5294, 8545, 7407, 1531, 28860, 28892, 24407, 5761, 28785, 1224, 5658, 4026, 2197, 28907, 3172, 1788, 1454, 1451, 7921, 28794, 842, 22744, 28870, 28773, 13418, 4086, 3553, 2509, 28788, 6971, 28763, 28810, 703, 7763, 2683, 1051, 15796, 1125, 6515, 613, 28750, 28740, 28723, 28734, 2807, 28852, 28844, 698, 6008, 28870, 28773, 27646, 698, 842, 5387, 613, 28782, 28734, 28723, 28740, 2807, 28844, 28859, 28705, 28740, 1051, 4015, 508, 842, 22744, 4522, 28773, 698, 14398, 2491, 853, 1695, 28788, 2749, 21865, 2978, 5991, 325, 10890, 16265, 3542, 3020, 28786, 28731, 28705, 28740, 28734, 28734, 28823, 28705, 28740, 28750, 28823, 28705, 28783, 28734, 28823, 28705, 28783, 28823, 3542, 28799, 5304, 3965, 13428, 19349, 2385, 1454, 13042, 28829, 649, 665, 1696, 28817, 8848, 853, 13099, 2676, 5304, 3965, 13428, 19349, 2385, 1454, 13042, 28829, 1619, 28799, 1051, 1131, 28795, 28817, 28733, 28740, 28734, 16810, 8848, 1586, 2290, 1125, 870, 2676, 2807, 28852, 28844, 698, 6008, 28870, 28773, 27646, 698, 842, 5387, 613, 28750, 28740, 28723, 28734, 6171, 1120, 28836, 23480, 16810, 3111, 1586, 2290, 1125, 22588, 2807, 28844, 28859, 28705, 28740, 1051, 4015, 508, 842, 315, 28782, 28734, 28723, 28740, 22202, 2676, 2683, 4627, 13014, 4026, 325, 10890, 16265, 3542, 3020, 28786, 28731, 28705, 28740, 28734, 28734, 28823, 28705, 28740, 28750, 28823, 28705, 28770, 28781, 28823, 28705, 28770, 28781, 28823, 28705, 28740, 28734, 28823, 28705, 28740, 28734, 28823, 26209, 8900, 16810, 3111, 1586, 2290, 1125, 22588, 17282, 16363, 1695, 28788, 2749, 21865, 2978, 5991, 20959, 28786, 11061, 4087, 2491, 26287, 28795, 1051, 2054, 28812, 800, 2491, 929, 1119, 1120, 917, 1696, 7265, 8848, 5304, 1175, 17942, 10603, 2676, 26287, 28795, 5234, 4942, 5555, 22588, 8257, 1454, 1451, 2454, 8067, 3111, 1051, 1621, 853, 9571, 28773, 1351, 2932, 28826, 27372, 2409, 28786], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df_labeled['text_features'],\n",
        "    df_labeled['mace_target'],\n",
        "    test_size=0.2, # 20% на тест\n",
        "    random_state=SEED,\n",
        "    stratify=df_labeled['mace_target']\n",
        ")\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_texts)}\")\n",
        "print(f\"Размер тестовой выборки: {len(test_texts)}\")\n",
        "print(f\"Распределение классов в обучающей выборке: {pd.Series(train_labels).value_counts(normalize=True)}\")\n",
        "print(f\"Распределение классов в тестовой выборке: {pd.Series(test_labels).value_counts(normalize=True)}\")\n",
        "\n",
        "# Создание словарей для Dataset\n",
        "train_data = {'text': list(train_texts), 'label': list(train_labels)}\n",
        "test_data = {'text': list(test_texts), 'label': list(test_labels)}\n",
        "\n",
        "# Преобразование в Hugging Face Dataset\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(\"\\nСтруктура датасета Hugging Face:\")\n",
        "print(raw_datasets)\n",
        "\n",
        "# Функция токенизации\n",
        "def tokenize_function(examples):\n",
        "    # Используем PRACTICAL_MAX_LENGTH, определенную ранее\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=PRACTICAL_MAX_LENGTH)\n",
        "    # padding=False здесь, т.к. DataCollatorWithPadding сделает это динамически для каждого батча\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data collator для динамического паддинга батчей до максимальной длины в батче\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "print(\"\\nПример токенизированного датасета:\")\n",
        "print(tokenized_datasets['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoT9Q1_RftQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ec89f0-d324-40c3-e8f6-dfd9d326a0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Модель после применения LoRA:\n",
            "trainable params: 20,979,712 || all params: 7,131,648,000 || trainable%: 0.2942\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Ранг LoRA-матриц.\n",
        "    lora_alpha=16, # Альфа для масштабирования.\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05, # Dropout для LoRA слоев\n",
        "    bias=\"none\", # Тип смещения.\n",
        "    task_type=TaskType.SEQ_CLS # Задача классификации последовательностей\n",
        ")\n",
        "\n",
        "# Применяем LoRA к модели\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"\\nМодель после применения LoRA:\")\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kiewsE5ftQa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import torch # Импортируем torch для проверки на NaN/Inf\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Добавляем печать логитов и меток для отладки\n",
        "    # print(\"\\n--- Logits and Labels for Debugging ---\")\n",
        "    # print(\"Labels:\", labels)\n",
        "    # print(\"Logits:\", logits)\n",
        "    # Проверка на NaN/Inf в логитах перед расчетом метрик\n",
        "    if isinstance(logits, np.ndarray):\n",
        "        if np.isnan(logits).any() or np.isinf(logits).any():\n",
        "            print(\"ПРЕДУПРЕЖДЕНИЕ: Logits содержат NaN/Inf!\")\n",
        "            # Возвращаем placeholder метрики, если логиты некорректны\n",
        "            return {\"accuracy\": 0.0, \"f1\": 0.0, \"auc\": 0.0}\n",
        "    elif isinstance(logits, torch.Tensor):\n",
        "         if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "            print(\"ПРЕДУПРЕЖДЕНИЕ: Logits содержат NaN/Inf!\")\n",
        "            # Возвращаем placeholder метрики, если логиты некорректны\n",
        "            return {\"accuracy\": 0.0, \"f1\": 0.0, \"auc\": 0.0}\n",
        "\n",
        "\n",
        "    # Если логиты корректны, продолжаем расчет метрик\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Убедимся, что labels и predictions имеют правильный тип для sklearn\n",
        "    labels = labels.astype(int) if hasattr(labels, 'astype') else labels\n",
        "    predictions = predictions.astype(int) if hasattr(predictions, 'astype') else predictions\n",
        "\n",
        "\n",
        "    # Проверяем количество уникальных меток для f1_score (binary требует 2 класса)\n",
        "    unique_labels = np.unique(labels)\n",
        "    if len(unique_labels) < 2:\n",
        "        print(f\"ПРЕДУПРЕЖДЕНИЕ: В eval_pred только один класс ({unique_labels}). Невозможно рассчитать бинарную f1 и AUC.\")\n",
        "        # Возвращаем метрики, которые можно посчитать, и 0 для остальных\n",
        "        acc = accuracy_score(labels, predictions)\n",
        "        return {\"accuracy\": acc, \"f1\": 0.0, \"auc\": 0.0}\n",
        "\n",
        "    # f1_score: 'binary' или 'weighted' в зависимости от задачи и распределения классов\n",
        "    # Если уверены, что задача бинарная и оба класса присутствуют в батче:\n",
        "    f1 = f1_score(labels, predictions, average='binary')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    # roc_auc_score: требует вероятности для положительного класса (обычно второй столбец логитов)\n",
        "    # Убедимся, что логиты имеют как минимум 2 столбца для AUC\n",
        "    if logits.shape[-1] < 2:\n",
        "         print(f\"ПРЕДУПРЕЖДЕНИЕ: Logits имеют форму {logits.shape}. Невозможно рассчитать AUC для бинарной классификации.\")\n",
        "         auc = 0.0\n",
        "    else:\n",
        "        # Применяем softmax или sigmoid к логитам, чтобы получить вероятности\n",
        "        # Для roc_auc_score часто достаточно использовать сами логиты для положительного класса\n",
        "        # Если модель выводит логиты (до softmax/sigmoid), используем логиты положительного класса.\n",
        "        # Убедимся, что используем правильный столбец для положительного класса (предполагаем 1)\n",
        "        try:\n",
        "            # Для roc_auc_score часто передают scores (логиты) или вероятности.\n",
        "            # Если модель выдает логиты для двух классов [score_neg, score_pos], используем score_pos\n",
        "            auc_scores = logits[:, 1]\n",
        "            auc = roc_auc_score(labels, auc_scores)\n",
        "        except Exception as e:\n",
        "             print(f\"Ошибка при расчете AUC: {e}\")\n",
        "             auc = 0.0\n",
        "\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": auc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQJSlv_QxIRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2888d82d-5dde-4a76-a24c-e9adc55a4495"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "peft_model.config.use_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOfDj0tGftQb"
      },
      "source": [
        "### Train only with targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxUS3PhAftQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe707826-6452-452f-ef41-8242bafc4829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-03273648c16b>:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = \"./results_biomistral_lora_mace\"\n",
        "LOGGING_DIR = './logs_biomistral_lora_mace'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=5, # Для малых данных\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", # Оценивать после каждой эпохи\n",
        "    save_strategy=\"epoch\",       # Сохранять модель после каждой эпохи\n",
        "    load_best_model_at_end=True, # Загрузить лучшую модель в конце обучения\n",
        "    metric_for_best_model=\"auc\",  # Метрика для определения лучшей модели\n",
        "    logging_dir=LOGGING_DIR,\n",
        "    logging_steps=1,            # Как часто логировать - ставим 1 для быстрого логгирования\n",
        "    warmup_steps=3, # Возвращаем небольшое количество warm-up шагов\n",
        "    report_to=\"tensorboard\",\n",
        "    fp16=False, # Возвращаем False для fp16\n",
        "    bf16=True,  # Возвращаем True для bf16\n",
        "    gradient_checkpointing=False,\n",
        "    max_grad_norm=0.3,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Регистрация хука для отслеживания NaN/Inf в градиентах\n",
        "def grad_hook_fn(name):\n",
        "    def hook(grad):\n",
        "        if grad is not None and (torch.isnan(grad).any() or torch.isinf(grad).any()):\n",
        "            print(f\"Обнаружен NaN/Inf градиент для параметра: {name}\")\n",
        "    return hook\n",
        "\n",
        "for name, param in peft_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.register_hook(grad_hook_fn(name))\n",
        "\n",
        "# Создание Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    # Используем только первый пример для тренировки и оценки\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74YFuvShftQb"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU29kuGYftQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74efa017-0891-48d1-e8a9-349c11c6de14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Отображение Warnings отключено.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"Отображение Warnings отключено.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6hPFKzlftQb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "d4e67d3d-aef8-44b2-aefb-1fa93863077e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Начинаем обучение модели...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/65 20:56 < 45:01, 0.02 it/s, Epoch 1.64/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>3.965306</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.703947</td>\n",
              "      <td>0.583385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 1:07:42, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>3.965306</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.703947</td>\n",
              "      <td>0.583385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.409100</td>\n",
              "      <td>1.147998</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.658009</td>\n",
              "      <td>0.642909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.270900</td>\n",
              "      <td>1.187723</td>\n",
              "      <td>0.597990</td>\n",
              "      <td>0.574468</td>\n",
              "      <td>0.686905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.212800</td>\n",
              "      <td>1.230512</td>\n",
              "      <td>0.577889</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.646480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>1.275662</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.672723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 318284291GF\n",
            "  train_loss               =      1.5845\n",
            "  train_runtime            =  1:08:44.20\n",
            "  train_samples_per_second =       0.964\n",
            "  train_steps_per_second   =       0.016\n",
            "\n",
            "Обучение завершено.\n"
          ]
        }
      ],
      "source": [
        "print(\"Начинаем обучение модели...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Сохранение метрик обучения\n",
        "train_metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", train_metrics)\n",
        "trainer.save_metrics(\"train\", train_metrics)\n",
        "\n",
        "print(\"\\nОбучение завершено.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDu6kxtLftQb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "c17ea7de-a483-454e-ab9e-1130d5745053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Оценка лучшей модели на тестовой выборке:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:50]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =      0.598\n",
            "  eval_auc                =     0.6869\n",
            "  eval_f1                 =     0.5745\n",
            "  eval_loss               =     1.1877\n",
            "  eval_runtime            = 0:00:54.89\n",
            "  eval_samples_per_second =      3.625\n",
            "  eval_steps_per_second   =      0.237\n",
            "\n",
            "Метрики на тестовой выборке:\n",
            "  eval_loss: 1.1877\n",
            "  eval_accuracy: 0.5980\n",
            "  eval_f1: 0.5745\n",
            "  eval_auc: 0.6869\n",
            "  eval_runtime: 54.8975\n",
            "  eval_samples_per_second: 3.6250\n",
            "  eval_steps_per_second: 0.2370\n",
            "  epoch: 5.0000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nОценка лучшей модели на тестовой выборке:\")\n",
        "eval_metrics = trainer.evaluate() # Оценит лучшую модель, т.к. load_best_model_at_end=True\n",
        "\n",
        "trainer.log_metrics(\"eval\", eval_metrics)\n",
        "trainer.save_metrics(\"eval\", eval_metrics)\n",
        "\n",
        "print(\"\\nМетрики на тестовой выборке:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8lphy-1ftQb"
      },
      "outputs": [],
      "source": [
        "ADAPTER_OUTPUT_DIR = \"/content/drive/My Drive/biomistral_lora_mace_adapter\" # Указываем путь на Google Диск\n",
        "peft_model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_OUTPUT_DIR) # Сохраняем токенизатор вместе с адаптером для удобства\n",
        "\n",
        "print(f\"\\nОбученный LoRA адаптер и токенизатор сохранены в: {ADAPTER_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh5ArOS6ftQd"
      },
      "source": [
        "### Inference on the full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g17NDBt2ftQd"
      },
      "outputs": [],
      "source": [
        "train_id, test_id, train_labels, test_labels = train_test_split(\n",
        "    df_labeled['patient_id'],\n",
        "    df_labeled['mace_target'],\n",
        "    test_size=0.20, # 20% на тест\n",
        "    random_state=SEED,\n",
        "    stratify=df_labeled['mace_target']\n",
        ")\n",
        "\n",
        "train_data_labels = {'text': list(train_texts), 'label': list(train_labels), 'patient_id': train_id}\n",
        "test_data_labels = {'text': list(test_texts), 'label': list(test_labels), 'patient_id': test_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hahBSKJjftQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "ed8054fe-8fda-437f-b5fe-fcbc8b0f6b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Получение предсказаний для обучающей выборки...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработано 795 записей из обучающей выборки.\n",
            "\n",
            "Получение предсказаний для тестовой выборки...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработано 199 записей из тестовой выборки.\n",
            "\n",
            "Итоговый DataFrame с предсказаниями (первые 5 строк):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                             patient_id dataset_type  mace_probability_score  \\\n",
              "0  DF948A19-5BFF-4C6E-BC54-2CD3E0E01BD8        train                0.484380   \n",
              "1  2029DEDD-88FA-40DD-B22B-7B477D6BF2A2        train                0.036220   \n",
              "2  202ACB3E-2444-4581-BF73-2643E65F3BCB        train                0.000386   \n",
              "3  865A14DA-D423-45D7-848E-D18A45D3F0EB        train                0.334589   \n",
              "4  A6B0BECD-5313-426C-9EC1-ACE66154BBCD        train                0.437824   \n",
              "\n",
              "   actual_mace_target  \n",
              "0                   1  \n",
              "1                   0  \n",
              "2                   0  \n",
              "3                   0  \n",
              "4                   1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3c198d7-9803-4965-8908-14875d8dc802\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>dataset_type</th>\n",
              "      <th>mace_probability_score</th>\n",
              "      <th>actual_mace_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DF948A19-5BFF-4C6E-BC54-2CD3E0E01BD8</td>\n",
              "      <td>train</td>\n",
              "      <td>0.484380</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2029DEDD-88FA-40DD-B22B-7B477D6BF2A2</td>\n",
              "      <td>train</td>\n",
              "      <td>0.036220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>202ACB3E-2444-4581-BF73-2643E65F3BCB</td>\n",
              "      <td>train</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>865A14DA-D423-45D7-848E-D18A45D3F0EB</td>\n",
              "      <td>train</td>\n",
              "      <td>0.334589</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A6B0BECD-5313-426C-9EC1-ACE66154BBCD</td>\n",
              "      <td>train</td>\n",
              "      <td>0.437824</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3c198d7-9803-4965-8908-14875d8dc802')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3c198d7-9803-4965-8908-14875d8dc802 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3c198d7-9803-4965-8908-14875d8dc802');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e5669be6-d269-4b39-a414-63899df7b419\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5669be6-d269-4b39-a414-63899df7b419')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e5669be6-d269-4b39-a414-63899df7b419 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"torch\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"patient_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2029DEDD-88FA-40DD-B22B-7B477D6BF2A2\",\n          \"A6B0BECD-5313-426C-9EC1-ACE66154BBCD\",\n          \"202ACB3E-2444-4581-BF73-2643E65F3BCB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mace_probability_score\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.03622005507349968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"actual_mace_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер итогового DataFrame: (994, 4)\n",
            "\n",
            "DataFrame с предсказаниями успешно сохранен в: /content/drive/My Drive/biomistral_mace_predictions.parquet\n"
          ]
        }
      ],
      "source": [
        "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
        "    trainer.model.eval()\n",
        "elif 'peft_model' in locals(): # Если trainer был удален, но peft_model осталась\n",
        "    peft_model.eval()\n",
        "else:\n",
        "    raise ValueError(\"Модель (trainer или peft_model) не найдена. Убедитесь, что обучение было завершено.\")\n",
        "\n",
        "# --- Предсказания для обучающей выборки ---\n",
        "print(\"\\nПолучение предсказаний для обучающей выборки...\")\n",
        "if \"train\" in tokenized_datasets:\n",
        "    train_pred_output = trainer.predict(tokenized_datasets[\"train\"])\n",
        "    train_logits = train_pred_output.predictions\n",
        "    # Преобразование логитов в вероятности для класса 1 (MACE)\n",
        "    train_probabilities_mace = torch.softmax(torch.tensor(train_logits), dim=-1)[:, 1].numpy()\n",
        "    # Исходные метки и ID пациентов из датасета\n",
        "    train_actual_labels = tokenized_datasets[\"train\"][\"label\"]\n",
        "    train_patient_ids_ds = train_data_labels[\"patient_id\"]\n",
        "\n",
        "    train_results_list = []\n",
        "    for i in range(len(train_patient_ids_ds)):\n",
        "        train_results_list.append({\n",
        "            'patient_id': train_patient_ids_ds.values[i],\n",
        "            'dataset_type': 'train',\n",
        "            'mace_probability_score': train_probabilities_mace[i],\n",
        "            'actual_mace_target': train_actual_labels[i]\n",
        "        })\n",
        "    print(f\"Обработано {len(train_results_list)} записей из обучающей выборки.\")\n",
        "else:\n",
        "    print(\"Обучающая выборка (tokenized_datasets['train']) не найдена.\")\n",
        "    train_results_list = []\n",
        "\n",
        "# --- Предсказания для тестовой выборки ---\n",
        "print(\"\\nПолучение предсказаний для тестовой выборки...\")\n",
        "if \"test\" in tokenized_datasets:\n",
        "    test_pred_output = trainer.predict(tokenized_datasets[\"test\"])\n",
        "    test_logits = test_pred_output.predictions\n",
        "    # Преобразование логитов в вероятности для класса 1 (MACE)\n",
        "    test_probabilities_mace = torch.softmax(torch.tensor(test_logits), dim=-1)[:, 1].numpy()\n",
        "    # Исходные метки и ID пациентов из датасета\n",
        "    test_actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
        "    test_patient_ids_ds = test_data_labels[\"patient_id\"]\n",
        "\n",
        "    test_results_list = []\n",
        "    for i in range(len(test_patient_ids_ds)):\n",
        "        test_results_list.append({\n",
        "            'patient_id': test_patient_ids_ds.values[i],\n",
        "            'dataset_type': 'test',\n",
        "            'mace_probability_score': test_probabilities_mace[i],\n",
        "            'actual_mace_target': test_actual_labels[i]\n",
        "        })\n",
        "    print(f\"Обработано {len(test_results_list)} записей из тестовой выборки.\")\n",
        "else:\n",
        "    print(\"Тестовая выборка (tokenized_datasets['test']) не найдена.\")\n",
        "    test_results_list = []\n",
        "\n",
        "# --- Объединение результатов в один DataFrame ---\n",
        "if train_results_list or test_results_list:\n",
        "    final_scores_df = pd.DataFrame(train_results_list + test_results_list)\n",
        "    print(\"\\nИтоговый DataFrame с предсказаниями (первые 5 строк):\")\n",
        "    display(final_scores_df.head())\n",
        "    print(f\"\\nРазмер итогового DataFrame: {final_scores_df.shape}\")\n",
        "\n",
        "    # --- Сохранение DataFrame на Google Диск ---\n",
        "    output_scores_filename = \"/content/drive/My Drive/biomistral_mace_predictions.parquet\"\n",
        "    try:\n",
        "        final_scores_df.to_parquet(output_scores_filename, index=False)\n",
        "        print(f\"\\nDataFrame с предсказаниями успешно сохранен в: {output_scores_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nОшибка при сохранении DataFrame: {e}\")\n",
        "else:\n",
        "    print(\"\\nНет данных для создания итогового DataFrame.\")\n",
        "\n",
        "# Очистка памяти\n",
        "import gc\n",
        "del train_pred_output, test_pred_output, train_logits, test_logits\n",
        "del train_probabilities_mace, test_probabilities_mace\n",
        "del final_scores_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeWPaBnXftQe"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f1H6vjwMXPNa",
        "MRUWeqJKXWX-",
        "QWweX0UpXkgb",
        "qq9BV88xgPeY",
        "25f9cc69"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1n3Zdz5sSwjg1rg_JbVoijsgj87wJsQOw",
      "authorship_tag": "ABX9TyOQ6QzHxIOlzDDFxGUbRsts"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}