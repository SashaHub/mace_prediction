{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "f1H6vjwMXPNa",
        "QWweX0UpXkgb"
      ],
      "mount_file_id": "1bKvKR9Oxy5iQA1s1aAc6ogww5Rts29Oo",
      "authorship_tag": "ABX9TyM2K+Z6AQo8OWEvL1xlm/DV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SashaHub/mace_prediction/blob/main/MedGemma_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qr3EOWrVYR2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and installations"
      ],
      "metadata": {
        "id": "f1H6vjwMXPNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "6B1za6EuneQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict\n",
        "!pip install transformers datasets accelerate peft bitsandbytes sentencepiece\n",
        "!pip install --upgrade --no-cache-dir --no-deps unsloth"
      ],
      "metadata": {
        "id": "7_wxKmx_XI5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN0T4wALW7Qb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import xmltodict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig # Import BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json # Для отладки и сохранения промежуточных данных, если нужно\n",
        "\n",
        "# --- Константы и определения ---\n",
        "\n",
        "# Коды МКБ-10 для MACE (можно расширить при необходимости)\n",
        "# Инфаркт миокарда (острый и повторный)\n",
        "MI_CODES = [f\"I21.{i}\" for i in range(10)] + \\\n",
        "           [f\"I21.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "           [f\"I22.{i}\" for i in range(10)] + \\\n",
        "           [f\"I22.{i}{j}\" for i in range(10) for j in range(10)]\n",
        "# Инсульты\n",
        "STROKE_CODES = [f\"I60.{i}\" for i in range(10)] + \\\n",
        "               [f\"I60.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I61.{i}\" for i in range(10)] + \\\n",
        "               [f\"I61.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I62.{i}\" for i in range(10)] + \\\n",
        "               [f\"I62.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I63.{i}\" for i in range(10)] + \\\n",
        "               [f\"I63.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I64\"] + \\\n",
        "               [f\"I64.{i}\" for i in range(10)] + \\\n",
        "               [f\"I64.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I65.{i}\" for i in range(10)] + \\\n",
        "               [f\"I65.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I66.{i}\" for i in range(10)] + \\\n",
        "               [f\"I66.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I67.{i}\" for i in range(10)] + \\\n",
        "               [f\"I67.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I68.{i}\" for i in range(10)] + \\\n",
        "               [f\"I68.{i}{j}\" for i in range(10) for j in range(10)] + \\\n",
        "               [f\"I69.{i}\" for i in range(10)] + \\\n",
        "               [f\"I69.{i}{j}\" for i in range(10) for j in range(10)]\n",
        "\n",
        "MACE_ICD_CODES = set(MI_CODES + STROKE_CODES)\n",
        "\n",
        "# Возможные отображения смерти в поле исхода госпитализации (может потребоваться дополнение)\n",
        "DEATH_DISPLAY_NAMES = [\"смерть\", \"умер\", \"умерла\", \"летальный исход\"]\n",
        "\n",
        "# Общий начальный путь для многих секций в структуре CDA\n",
        "# ['ClinicalDocument', 'component', 'structuredBody', 'component', 'section']\n",
        "BASE_PATH_STRUCTURED_BODY_COMP_SECTION = ['ClinicalDocument', 'component', 'structuredBody', 'component']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "MRUWeqJKXWX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_section_by_path(data_dict, path, default=None):\n",
        "    \"\"\"\n",
        "    Безопасно извлекает значение из вложенного словаря по списку ключей/индексов.\n",
        "    Args:\n",
        "        data_dict (dict): Словарь с данными документа.\n",
        "        path (list): Список ключей и/или индексов для навигации.\n",
        "        default: Значение по умолчанию, если путь не найден.\n",
        "    Returns:\n",
        "        Извлеченное значение или default.\n",
        "    \"\"\"\n",
        "    current = data_dict\n",
        "    try:\n",
        "        for key_or_index in path:\n",
        "            if isinstance(current, list):\n",
        "                current = current[key_or_index]\n",
        "            else:\n",
        "                current = current[key_or_index]\n",
        "        return current\n",
        "    except (KeyError, IndexError, TypeError):\n",
        "        return default\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Очищает текст от лишних символов, множественных пробелов, заменяет плейсхолдеры.\n",
        "    Сохраняет цифры, точки, запятые, дефисы, русские и латинские буквы, знаки процента, скобки, слеши.\n",
        "    Args:\n",
        "        text (str): Входная строка.\n",
        "    Returns:\n",
        "        str: Очищенная строка.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.replace('!!!!!!!!!!!!!', ' ')\n",
        "    text = text.replace('<.>', ' ')\n",
        "    text = re.sub(r'[^\\w\\s\\.,/\\-\\%()\\<\\>]', ' ', text, flags=re.UNICODE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def parse_xml_file(file_path):\n",
        "    \"\"\"\n",
        "    Читает XML файл и конвертирует его в словарь Python.\n",
        "    Args:\n",
        "        file_path (str): Путь к XML файлу.\n",
        "    Returns:\n",
        "        dict: Словарь, представляющий XML, или None при ошибке.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            xml_content = f.read()\n",
        "        xml_content = re.sub(r'<\\?xml-stylesheet.*?\\?>', '', xml_content)\n",
        "        data_dict = xmltodict.parse(xml_content)\n",
        "        return data_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка парсинга XML файла {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_table_text_from_html_like(text_node):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое из HTML-подобной таблицы внутри XML-узла <text>.\n",
        "    Args:\n",
        "        text_node: Узел <text> из XML, преобразованный в словарь.\n",
        "    Returns:\n",
        "        str: Конкатенированный текст из ячеек таблицы.\n",
        "    \"\"\"\n",
        "    if not text_node or not isinstance(text_node, dict):\n",
        "        return \"\"\n",
        "    all_text_parts = []\n",
        "    def recurse_extract(element):\n",
        "        if isinstance(element, str):\n",
        "            cleaned = clean_text(element)\n",
        "            if cleaned: all_text_parts.append(cleaned)\n",
        "        elif isinstance(element, dict):\n",
        "            if '#text' in element:\n",
        "                cleaned = clean_text(element['#text'])\n",
        "                if cleaned: all_text_parts.append(cleaned)\n",
        "            for key in element:\n",
        "                if key != '#text':\n",
        "                    recurse_extract(element[key])\n",
        "        elif isinstance(element, list):\n",
        "            for item in element:\n",
        "                recurse_extract(item)\n",
        "    recurse_extract(text_node)\n",
        "    return \" \".join(all_text_parts)"
      ],
      "metadata": {
        "id": "Z6n_XrhHXYAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patient_id(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает идентификатор пациента из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> id (первый элемент списка) -> @extension.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: ID пациента или None, если идентификатор не найден по указанному пути.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'id', 0, '@extension'])\n",
        "\n",
        "def get_patient_sex(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает пол пациента из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> patient -> administrativeGenderCode -> @displayName.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Пол пациента (например, \"Мужской\", \"Женский\") или None, если информация о поле не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'patient', 'administrativeGenderCode', '@displayName'])\n",
        "\n",
        "def get_patient_birth_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату рождения пациента в виде строки из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> recordTarget -> patientRole -> patient -> birthTime -> @value.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата рождения в формате \"YYYYMMDD...\" или None, если дата рождения не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'recordTarget', 'patientRole', 'patient', 'birthTime', '@value'])\n",
        "\n",
        "def get_admission_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату поступления в стационар в виде строки из структуры документа CDA.\n",
        "    Проверяет два возможных пути, так как дата может находиться в разных секциях:\n",
        "    1. ClinicalDocument -> documentationOf -> serviceEvent -> effectiveTime -> low -> @value\n",
        "    2. ClinicalDocument -> componentOf -> encompassingEncounter -> effectiveTime -> low -> @value\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата поступления в формате \"YYYYMMDD...\" или None, если дата не найдена по обоим путям.\n",
        "    \"\"\"\n",
        "    date_val = find_section_by_path(doc_dict, ['ClinicalDocument', 'documentationOf', 'serviceEvent', 'effectiveTime', 'low', '@value'])\n",
        "    if date_val:\n",
        "        return date_val\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'effectiveTime', 'low', '@value'])\n",
        "\n",
        "def get_discharge_date_str(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает дату выписки из стационара в виде строки из структуры документа CDA.\n",
        "    Проверяет два возможных пути, аналогично дате поступления, но использует 'high' вместо 'low':\n",
        "    1. ClinicalDocument -> documentationOf -> serviceEvent -> effectiveTime -> high -> @value\n",
        "    2. ClinicalDocument -> componentOf -> encompassingEncounter -> effectiveTime -> high -> @value\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Дата выписки в формате \"YYYYMMDD...\" или None, если дата не найдена по обоим путям.\n",
        "    \"\"\"\n",
        "    date_val = find_section_by_path(doc_dict, ['ClinicalDocument', 'documentationOf', 'serviceEvent', 'effectiveTime', 'high', '@value'])\n",
        "    if date_val:\n",
        "        return date_val\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'effectiveTime', 'high', '@value'])\n",
        "\n",
        "def get_age_at_admission(birth_date_str, admission_date_str):\n",
        "    \"\"\"\n",
        "    Вычисляет возраст пациента в полных годах на момент поступления.\n",
        "    Для вычисления используются только год, месяц и день из предоставленных строк дат.\n",
        "    Args:\n",
        "        birth_date_str (str): Строка с датой рождения в формате \"YYYYMMDD...\" (или None).\n",
        "        admission_date_str (str): Строка с датой поступления в формате \"YYYYMMDD...\" (или None).\n",
        "    Returns:\n",
        "        int: Возраст в полных годах или None, если одна из дат отсутствует или имеет некорректный формат.\n",
        "    \"\"\"\n",
        "    if not birth_date_str or not admission_date_str:\n",
        "        return None\n",
        "    try:\n",
        "        # Парсим только первые 8 символов (YYYYMMDD)\n",
        "        birth_date = datetime.strptime(birth_date_str[:8], \"%Y%m%d\")\n",
        "        admission_date = datetime.strptime(admission_date_str[:8], \"%Y%m%d\")\n",
        "        # Вычисляем возраст: разница лет минус 1, если день рождения в текущем году еще не наступил\n",
        "        age = admission_date.year - birth_date.year - ((admission_date.month, admission_date.day) < (birth_date.month, birth_date.day))\n",
        "        return age\n",
        "    except ValueError:\n",
        "        # Возвращаем None, если формат даты некорректен для парсинга\n",
        "        return None\n",
        "\n",
        "def find_section_by_display_name(doc_dict, base_path_list, target_display_name):\n",
        "    \"\"\"\n",
        "    Находит и извлекает текстовое содержимое секции по ее @displayName в структуре документа CDA.\n",
        "    Рекурсивно ищет на двух уровнях вложенности секций, начиная с указанного базового пути.\n",
        "    Пытается извлечь текст из '#text', напрямую из строки или из HTML-подобной структуры (таблицы).\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "        base_path_list (list): Список ключей/индексов, указывающий базовый путь к компонентам,\n",
        "                               которые могут содержать искомые секции (например, путь к structuredBody/component).\n",
        "        target_display_name (str): Значение атрибута @displayName искомой секции.\n",
        "    Returns:\n",
        "        str: Очищенное текстовое содержимое найденной секции или пустая строка, если секция не найдена\n",
        "             или не содержит извлекаемого текста.\n",
        "    \"\"\"\n",
        "    # Извлекаем основные компоненты, которые могут содержать секции, используя безопасный поиск\n",
        "    main_components = find_section_by_path(doc_dict, base_path_list, [])\n",
        "    # Убеждаемся, что main_components является списком для удобства итерации\n",
        "    if not isinstance(main_components, list):\n",
        "        main_components = [main_components]\n",
        "\n",
        "    # Обходим компоненты первого уровня вложенности\n",
        "    for comp_level1 in main_components:\n",
        "        # Проверяем, что компонент существует и содержит ключ 'section'\n",
        "        if not comp_level1 or 'section' not in comp_level1: continue\n",
        "        section_level1 = comp_level1['section']\n",
        "\n",
        "        sections_to_check_lvl1 = []\n",
        "        # Преобразуем section_level1 в список, если это не список (для случая одной секции)\n",
        "        if isinstance(section_level1, list): sections_to_check_lvl1.extend(section_level1)\n",
        "        elif isinstance(section_level1, dict): sections_to_check_lvl1.append(section_level1)\n",
        "\n",
        "        # Обходим секции первого уровня\n",
        "        for sec_l1_dict in sections_to_check_lvl1:\n",
        "            if not sec_l1_dict: continue\n",
        "            # Проверяем, совпадает ли @displayName текущей секции с целевым\n",
        "            if sec_l1_dict.get('code', {}).get('@displayName') == target_display_name:\n",
        "                # Если нашли секцию, пытаемся извлечь ее текстовое содержимое\n",
        "                text_node = sec_l1_dict.get('text')\n",
        "                # Проверяем разные возможные места хранения текста: '#text', прямой текст, entry/observation/value, HTML-подобная таблица\n",
        "                if isinstance(text_node, dict) and '#text' in text_node: return clean_text(text_node['#text'])\n",
        "                if isinstance(text_node, str): return clean_text(text_node)\n",
        "                entry_val = find_section_by_path(sec_l1_dict, ['entry', 'observation', 'value'])\n",
        "                if isinstance(entry_val, dict) and '#text' in entry_val: return clean_text(entry_val['#text'])\n",
        "                if isinstance(entry_val, str): return clean_text(entry_val)\n",
        "                content_val = find_section_by_path(text_node, ['content'])\n",
        "                if content_val: return extract_table_text_from_html_like(text_node)\n",
        "\n",
        "            # Если секция первого уровня не целевая, проверяем ее вложенные компоненты (второй уровень)\n",
        "            components_level2 = find_section_by_path(sec_l1_dict, ['component'], [])\n",
        "            # Убеждаемся, что components_level2 является списком\n",
        "            if not isinstance(components_level2, list):\n",
        "                components_level2 = [components_level2]\n",
        "\n",
        "            # Обходим компоненты второго уровня\n",
        "            for comp_level2 in components_level2:\n",
        "                # Проверяем, что компонент существует и содержит ключ 'section'\n",
        "                if not comp_level2 or 'section' not in comp_level2: continue\n",
        "                section_level2 = comp_level2['section']\n",
        "                sections_to_check_lvl2 = []\n",
        "                # Преобразуем section_level2 в список\n",
        "                if isinstance(section_level2, list): sections_to_check_lvl2.extend(section_level2)\n",
        "                elif isinstance(section_level2, dict): sections_to_check_lvl2.append(section_level2)\n",
        "\n",
        "                # Обходим секции второго уровня\n",
        "                for sec_l2_dict in sections_to_check_lvl2:\n",
        "                    if not sec_l2_dict: continue\n",
        "                    # Проверяем, совпадает ли @displayName текущей секции с целевым\n",
        "                    if sec_l2_dict.get('code', {}).get('@displayName') == target_display_name:\n",
        "                        # Если нашли секцию, пытаемся извлечь ее текстовое содержимое (аналогично первому уровню)\n",
        "                        text_node = sec_l2_dict.get('text')\n",
        "                        if isinstance(text_node, dict) and '#text' in text_node: return clean_text(text_node['#text'])\n",
        "                        if isinstance(text_node, str): return clean_text(text_node)\n",
        "                        entry_val = find_section_by_path(sec_l2_dict, ['entry', 'observation', 'value'])\n",
        "                        if isinstance(entry_val, dict) and '#text' in entry_val: return clean_text(entry_val['#text'])\n",
        "                        if isinstance(entry_val, str): return clean_text(entry_val)\n",
        "                        content_val = find_section_by_path(text_node, ['content'])\n",
        "                        if content_val: return extract_table_text_from_html_like(text_node)\n",
        "    return \"\" # Возвращаем пустую строку, если секция с целевым @displayName не найдена на обоих уровнях.\n",
        "\n",
        "def get_anamnesis_disease(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Анамнез заболевания\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст анамнеза заболевания или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Анамнез заболевания\")\n",
        "\n",
        "def get_anamnesis_life(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Анамнез жизни\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст анамнеза жизни или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Анамнез жизни\")\n",
        "\n",
        "def get_condition_on_admission(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Состояние при поступлении\" по ее @displayName.\n",
        "    Использует find_section_by_display_name с базовым путем к структурированному телу документа.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст описания состояния при поступлении или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    return find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Состояние при поступлении\")\n",
        "\n",
        "def get_diagnoses_from_section(section_dict, section_title_target):\n",
        "    \"\"\"\n",
        "    Извлекает список диагнозов (код МКБ и текст описания) из заданной секции документа CDA.\n",
        "    Функция ищет вложенные секции с @displayName=\"Диагнозы\" и проверяет их заголовок на соответствие section_title_target.\n",
        "    Диагнозы извлекаются как из структуры <entry>/<observation>, так и из HTML-подобных таблиц в <text>.\n",
        "    Args:\n",
        "        section_dict (dict): Словарь, представляющий родительскую секцию, которая может содержать вложенные секции \"Диагнозы\".\n",
        "        section_title_target (str): Ожидаемое начало текста заголовка вложенной секции \"Диагнозы\"\n",
        "                                    (например, \"Установленные диагнозы при поступлении\").\n",
        "    Returns:\n",
        "        list: Список словарей, каждый из которых представляет диагноз: {'mkb_code': '...', 'text': '...'}.\n",
        "              Возвращает пустой список, если диагнозы не найдены.\n",
        "    \"\"\"\n",
        "    diagnoses = []\n",
        "    # Ищем вложенные компоненты внутри родительской секции\n",
        "    components = find_section_by_path(section_dict, ['component'], [])\n",
        "    # Убеждаемся, что components является списком\n",
        "    if not isinstance(components, list): components = [components]\n",
        "\n",
        "    # Обходим вложенные компоненты\n",
        "    for comp in components:\n",
        "        diag_section = comp.get('section', {})\n",
        "        # Проверяем, является ли вложенная секция секцией диагнозов с нужным @displayName и заголовком\n",
        "        if diag_section.get('code', {}).get('@displayName') == \"Диагнозы\" and \\\n",
        "           clean_text(find_section_by_path(diag_section,['title','#text'],\"\")).startswith(section_title_target):\n",
        "\n",
        "            # --- Извлечение диагнозов из структуры <entry>/<observation> ---\n",
        "            entries = find_section_by_path(diag_section, ['entry'], [])\n",
        "            # Убеждаемся, что entries является списком\n",
        "            if not isinstance(entries, list): entries = [entries]\n",
        "            for entry_item in entries:\n",
        "                # Пропускаем записи с определенным codeSystem (часто служебные или недиагнозы)\n",
        "                if find_section_by_path(entry_item, ['act', 'code', '@codeSystem']) == \"1.2.643.5.1.13.13.99.2.795\":\n",
        "                    continue\n",
        "                # Диагноз может быть вложен в entryRelationship/observation\n",
        "                obs_list = find_section_by_path(entry_item, ['observation', 'entryRelationship', 'observation'], [])\n",
        "                # Убеждаемся, что obs_list является списком\n",
        "                if not isinstance(obs_list, list): obs_list = [obs_list]\n",
        "                for obs in obs_list:\n",
        "                    if not obs: continue\n",
        "                    # Извлекаем код МКБ (@code) и текст диагноза (#text или @displayName)\n",
        "                    icd_code = find_section_by_path(obs, ['value', '@code'])\n",
        "                    diag_text = clean_text(find_section_by_path(obs, ['text', '#text']))\n",
        "                    if not diag_text: # Если текст не найден в <text>, ищем в @displayName value\n",
        "                        diag_text = clean_text(find_section_by_path(obs, ['value', '@displayName']))\n",
        "                    if icd_code and diag_text:\n",
        "                        diagnoses.append({'mkb_code': icd_code.strip(), 'text': diag_text.strip()})\n",
        "\n",
        "            # --- Извлечение диагнозов из HTML-подобной таблицы в <text> ---\n",
        "            text_node = diag_section.get('text')\n",
        "            if text_node and 'table' in text_node:\n",
        "                table_data = text_node['table']\n",
        "                tbody = table_data.get('tbody')\n",
        "                if tbody and 'tr' in tbody:\n",
        "                    rows = tbody['tr']\n",
        "                    # Убеждаемся, что rows является списком строк\n",
        "                    if not isinstance(rows, list): rows = [rows]\n",
        "                    for row in rows:\n",
        "                        cols = row.get('td') # Ячейки таблицы\n",
        "                        # Проверяем, что это список ячеек и их достаточно (номер, описание, код)\n",
        "                        if isinstance(cols, list) and len(cols) >= 3:\n",
        "                            # Извлекаем описание и код из соответствующих ячеек, проверяя разные пути к тексту\n",
        "                            col1_content = find_section_by_path(cols[1], ['content', '#text']) or find_section_by_path(cols[1],['content']) or find_section_by_path(cols[1],['#text'])\n",
        "                            col2_content = find_section_by_path(cols[2], ['content', '#text']) or find_section_by_path(cols[2],['content']) or find_section_by_path(cols[2],['#text'])\n",
        "                            desc_text = clean_text(col1_content)\n",
        "                            code_text = clean_text(col2_content)\n",
        "                            if code_text and desc_text:\n",
        "                                 diagnoses.append({'mkb_code': code_text.strip(), 'text': desc_text.strip()})\n",
        "    return diagnoses\n",
        "\n",
        "def get_all_diagnoses(doc_dict, admission_or_discharge=\"admission\"):\n",
        "    \"\"\"\n",
        "    Извлекает все диагнозы (при поступлении или при выписке) из документа CDA.\n",
        "    Функция ищет диагнозы в различных стандартных секциях (\"Состояние при поступлении\",\n",
        "    \"Пребывание в стационаре\", \"Состояние при выписке\") и их вложенных секциях \"Диагнозы\".\n",
        "    Также включает логику извлечения диагнозов из таблиц в секции \"Пребывание в стационаре\".\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "        admission_or_discharge (str): Указывает, какие диагнозы искать: \"admission\" для диагнозов при поступлении,\n",
        "                                      \"discharge\" для диагнозов при выписке.\n",
        "    Returns:\n",
        "        list: Список уникальных словарей {'mkb_code': '...', 'text': '...'} найденных диагнозов.\n",
        "              Уникальность определяется по коду МКБ (без подкатегории) и тексту диагноза.\n",
        "              Возвращает пустой список, если диагнозы не найдены.\n",
        "    \"\"\"\n",
        "    all_diagnoses_list = []\n",
        "    # Ищем контейнеры секций в структурированном теле документа\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Определяем целевые названия родительских секций и заголовков вложенных секций \"Диагнозы\"\n",
        "    # в зависимости от того, ищем ли мы диагнозы при поступлении или при выписке.\n",
        "    target_parent_section_name = \"Пребывание в стационаре\" if admission_or_discharge == \"admission\" else \"Состояние при выписке\"\n",
        "    target_diag_section_title = \"Установленные диагнозы при поступлении\" if admission_or_discharge == \"admission\" else \"Установленные диагнозы при выписке\"\n",
        "\n",
        "    # Обходим контейнеры секций\n",
        "    for section_container_item in section_containers:\n",
        "        if not section_container_item or 'section' not in section_container_item: continue # Пропускаем пустые или без секций\n",
        "        current_section_candidates = section_container_item['section']\n",
        "        # Убеждаемся, что current_section_candidates является списком\n",
        "        if not isinstance(current_section_candidates, list): current_section_candidates = [current_section_candidates]\n",
        "\n",
        "        # Обходим секции первого уровня в контейнере\n",
        "        for current_section_dict in current_section_candidates:\n",
        "            if not current_section_dict: continue\n",
        "            # Получаем @displayName текущей секции\n",
        "            parent_display_name = current_section_dict.get('code', {}).get('@displayName')\n",
        "\n",
        "            # Если это секция \"Состояние при поступлении\" и мы ищем диагнозы при поступлении\n",
        "            if parent_display_name == \"Состояние при поступлении\" and admission_or_discharge == \"admission\":\n",
        "                 # Извлекаем диагнозы из этой секции, используя целевой заголовок\n",
        "                 diagnoses = get_diagnoses_from_section(current_section_dict, target_diag_section_title)\n",
        "                 all_diagnoses_list.extend(diagnoses)\n",
        "            # Если это целевая родительская секция (\"Пребывание в стационаре\" или \"Состояние при выписке\")\n",
        "            elif parent_display_name == target_parent_section_name:\n",
        "                # Извлекаем диагнозы из этой секции, используя целевой заголовок\n",
        "                diagnoses = get_diagnoses_from_section(current_section_dict, target_diag_section_title)\n",
        "                all_diagnoses_list.extend(diagnoses)\n",
        "            # Специальная логика для извлечения диагнозов из таблицы в секции \"Пребывание в стационаре\"\n",
        "            # (часто там указываются основные диагнозы пребывания)\n",
        "            if admission_or_discharge == \"admission\" and parent_display_name == \"Пребывание в стационаре\":\n",
        "                text_node_main = current_section_dict.get('text')\n",
        "                if text_node_main and 'table' in text_node_main:\n",
        "                    # Проверяем заголовки таблицы, чтобы убедиться, что это таблица диагнозов\n",
        "                    headers_text = extract_table_text_from_html_like(text_node_main.get('table',{}).get('thead',{}))\n",
        "                    if \"Вид нозологической единицы\" in headers_text and \"Код по МКБ-10\" in headers_text:\n",
        "                        table_data = text_node_main['table']\n",
        "                        tbody = table_data.get('tbody')\n",
        "                        if tbody and 'tr' in tbody:\n",
        "                            rows = tbody['tr']\n",
        "                            # Убеждаемся, что rows является списком строк\n",
        "                            if not isinstance(rows, list): rows = [rows]\n",
        "                            for row in rows:\n",
        "                                cols = row.get('td') # Ячейки таблицы\n",
        "                                # Проверяем, что это список ячеек и их достаточно\n",
        "                                if isinstance(cols, list) and len(cols) >= 3:\n",
        "                                    # Извлекаем описание и код из соответствующих ячеек\n",
        "                                    desc_node_val = find_section_by_path(cols[1], ['content', '#text']) or find_section_by_path(cols[1],['content']) or find_section_by_path(cols[1],['#text'])\n",
        "                                    code_node_val = find_section_by_path(cols[2], ['content', '#text']) or find_section_by_path(cols[2],['content']) or find_section_by_path(cols[2],['#text'])\n",
        "                                    desc_text = clean_text(desc_node_val)\n",
        "                                    code_text = clean_text(code_node_val)\n",
        "                                    if code_text and desc_text:\n",
        "                                        all_diagnoses_list.append({'mkb_code': code_text, 'text': desc_text})\n",
        "\n",
        "    # Удаляем дубликаты диагнозов. Уникальность определяется по коду МКБ (берем только основную часть) и тексту.\n",
        "    unique_diagnoses = []\n",
        "    seen_tuples = set()\n",
        "    for d in all_diagnoses_list:\n",
        "        mkb_code_cleaned = d['mkb_code'].split()[0] if d['mkb_code'] else \"\" # Берем только часть кода до первого пробела (например, \"I21.4\" из \"I21.4 MACE\")\n",
        "        if mkb_code_cleaned and (mkb_code_cleaned, d['text']) not in seen_tuples:\n",
        "            unique_diagnoses.append({'mkb_code': mkb_code_cleaned, 'text': d['text']})\n",
        "            seen_tuples.add((mkb_code_cleaned, d['text']))\n",
        "    return unique_diagnoses\n",
        "\n",
        "def get_discharge_outcome(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает исход госпитализации из структуры документа CDA.\n",
        "    Путь: ClinicalDocument -> componentOf -> encompassingEncounter -> dischargeDispositionCode -> @displayName.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Описание исхода госпитализации (@displayName, например \"выписан\", \"смерть\")\n",
        "             или None, если исход не найден.\n",
        "    \"\"\"\n",
        "    return find_section_by_path(doc_dict, ['ClinicalDocument', 'componentOf', 'encompassingEncounter', 'dischargeDispositionCode', '@displayName'])\n",
        "\n",
        "def get_instrumental_studies_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секций с инструментальными исследованиями из документа CDA.\n",
        "    Функция ищет секции внутри структуры 'PATIENTROUTE' (маршрут пациента) и 'PROC' (процедуры).\n",
        "    Собирает названия отделений, исследований, текст протоколов/результатов (включая таблицы)\n",
        "    и детали исследований из entry/observation.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Объединенный текст из всех найденных секций исследований, очищенный.\n",
        "             Возвращает пустую строку, если секции исследований не найдены.\n",
        "    \"\"\"\n",
        "    studies_texts = []\n",
        "    # Ищем контейнеры секций в структурированном теле документа\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Обходим контейнеры секций\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        top_level_section = sc['section']\n",
        "        sections_to_scan_for_patientroute = []\n",
        "        # Преобразуем top_level_section в список\n",
        "        if isinstance(top_level_section, list): sections_to_scan_for_patientroute.extend(top_level_section)\n",
        "        else: sections_to_scan_for_patientroute.append(top_level_section)\n",
        "\n",
        "        # Ищем секцию с кодом 'PATIENTROUTE' (маршрут пациента)\n",
        "        for s_l1 in sections_to_scan_for_patientroute:\n",
        "            if not s_l1: continue\n",
        "            if find_section_by_path(s_l1, ['code', '@code']) == 'PATIENTROUTE':\n",
        "                # Ищем вложенные компоненты (обычно отделения или этапы маршрута)\n",
        "                departments_components = find_section_by_path(s_l1, ['component'], [])\n",
        "                # Убеждаемся, что departments_components является списком\n",
        "                if not isinstance(departments_components, list): departments_components = [departments_components]\n",
        "                for dept_comp in departments_components:\n",
        "                    dept_section = dept_comp.get('section')\n",
        "                    if not dept_section: continue\n",
        "                    # Извлекаем название отделения/этапа\n",
        "                    dept_name = clean_text(find_section_by_path(dept_section, ['title', '#text']))\n",
        "                    if dept_name: studies_texts.append(f\"Отделение: {dept_name}\")\n",
        "\n",
        "                    # Ищем вложенные компоненты с секциями процедур ('PROC')\n",
        "                    researches_main_component = find_section_by_path(dept_section, ['component'], [])\n",
        "                    # Убеждаемся, что researches_main_component является списком\n",
        "                    if not isinstance(researches_main_component, list): researches_main_component = [researches_main_component]\n",
        "                    for res_main_comp_item in researches_main_component:\n",
        "                        proc_section = res_main_comp_item.get('section')\n",
        "                        # Проверяем, что это секция процедур\n",
        "                        if not proc_section or find_section_by_path(proc_section,['code','@code']) != 'PROC':\n",
        "                            continue\n",
        "                        # Ищем вложенные компоненты с деталями исследований\n",
        "                        actual_researches_components = find_section_by_path(proc_section, ['component'],[])\n",
        "                        # Убеждаемся, что actual_researches_components является списком\n",
        "                        if not isinstance(actual_researches_components, list): actual_researches_components = [actual_researches_components]\n",
        "                        for actual_res_comp in actual_researches_components:\n",
        "                            research_detail_section = actual_res_comp.get('section')\n",
        "                            if not research_detail_section: continue\n",
        "                            # Извлекаем название исследования\n",
        "                            research_name = clean_text(find_section_by_path(research_detail_section, ['title', '#text']))\n",
        "                            if research_name: studies_texts.append(f\"Исследование ({dept_name if dept_name else 'Общее'}): {research_name}\")\n",
        "\n",
        "                            # Извлекаем текст протокола/результатов из <text> (часто в виде таблиц)\n",
        "                            text_node = research_detail_section.get('text')\n",
        "                            table_text = extract_table_text_from_html_like(text_node)\n",
        "                            if table_text: studies_texts.append(f\"Протокол/Результаты: {table_text}\")\n",
        "\n",
        "                            # Извлекаем детали исследования из entry/observation\n",
        "                            entries = find_section_by_path(research_detail_section, ['entry'], [])\n",
        "                            # Убеждаемся, что entries является списком\n",
        "                            if not isinstance(entries, list): entries = [entries]\n",
        "                            for entry in entries:\n",
        "                                # Извлекаем название параметра (@displayName code) и его значение (#text value или @displayName value)\n",
        "                                obs_code_dn = clean_text(find_section_by_path(entry, ['observation', 'code', '@displayName']))\n",
        "                                obs_value_text = clean_text(find_section_by_path(entry, ['observation', 'value', '#text']))\n",
        "                                obs_value_dn = clean_text(find_section_by_path(entry, ['observation', 'value', '@displayName']))\n",
        "                                if obs_code_dn and obs_value_text:\n",
        "                                    studies_texts.append(f\"{obs_code_dn}: {obs_value_text}\")\n",
        "                                elif obs_code_dn and obs_value_dn and obs_value_dn != obs_code_dn : # Добавляем значение, если оно отличается от названия параметра\n",
        "                                    studies_texts.append(f\"{obs_code_dn}: {obs_value_dn}\")\n",
        "                                elif obs_value_text: # Если нет названия параметра, добавляем только текстовое значение\n",
        "                                    studies_texts.append(f\"Деталь исследования: {obs_value_text}\")\n",
        "\n",
        "    # Объединяем все собранные текстовые части исследований в одну строку, удаляя пустые\n",
        "    return \" \".join(filter(None, studies_texts))\n",
        "\n",
        "def get_general_hospitalization_info_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции с общими сведениями о госпитализации ('HOSP').\n",
        "    Ищет секцию по ее коду 'HOSP' на первом уровне вложенности в структурированном теле.\n",
        "    Извлекает текст из HTML-подобной структуры внутри <text>.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст общих сведений о госпитализации или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "    # Обходим контейнеры секций\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        hosp_section_candidates = sc['section']\n",
        "        # Убеждаемся, что hosp_section_candidates является списком\n",
        "        if not isinstance(hosp_section_candidates, list): hosp_section_candidates = [hosp_section_candidates]\n",
        "        # Ищем секцию с кодом 'HOSP'\n",
        "        for hs in hosp_section_candidates:\n",
        "            if hs and find_section_by_path(hs,['code','@code']) == 'HOSP':\n",
        "                 text_node = hs.get('text')\n",
        "                 # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                 return extract_table_text_from_html_like(text_node)\n",
        "    return \"\" # Возвращаем пустую строку, если секция не найдена\n",
        "\n",
        "def get_discharge_condition_text(doc_dict):\n",
        "    \"\"\"\n",
        "    Извлекает текстовое содержимое секции \"Состояние при выписке\".\n",
        "    Ищет секцию как по @displayName=\"Состояние при выписке\", так и по коду 'STATEDIS'.\n",
        "    По коду 'STATEDIS' ищет на первом уровне вложенности и вложенную в секцию 'HOSP'.\n",
        "    Извлекает текст из HTML-подобной структуры внутри <text>.\n",
        "    Args:\n",
        "        doc_dict (dict): Словарь, представляющий XML документа CDA.\n",
        "    Returns:\n",
        "        str: Очищенный текст описания состояния при выписке или пустая строка, если секция не найдена.\n",
        "    \"\"\"\n",
        "    # Сначала пробуем найти по display name (менее надежно, но может сработать)\n",
        "    text_content = find_section_by_display_name(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, \"Состояние при выписке\")\n",
        "\n",
        "    section_containers = find_section_by_path(doc_dict, BASE_PATH_STRUCTURED_BODY_COMP_SECTION, [])\n",
        "    # Убеждаемся, что section_containers является списком\n",
        "    if not isinstance(section_containers, list): section_containers = [section_containers]\n",
        "\n",
        "    # Обходим контейнеры секций для поиска по коду 'STATEDIS'\n",
        "    for sc in section_containers:\n",
        "        if not sc or 'section' not in sc: continue # Пропускаем пустые или без секций\n",
        "        current_level1_sections = sc['section']\n",
        "        # Убеждаемся, что current_level1_sections является списком\n",
        "        if not isinstance(current_level1_sections, list): current_level1_sections = [current_level1_sections]\n",
        "\n",
        "        # Ищем секцию с кодом 'STATEDIS' на первом уровне\n",
        "        for sec_l1 in current_level1_sections:\n",
        "            if sec_l1 and find_section_by_path(sec_l1,['code','@code']) == 'STATEDIS':\n",
        "                 text_node = sec_l1.get('text')\n",
        "                 # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                 return extract_table_text_from_html_like(text_node)\n",
        "            # Ищем секцию с кодом 'STATEDIS' вложенную в секцию 'HOSP'\n",
        "            if sec_l1 and find_section_by_path(sec_l1,['code','@code']) == 'HOSP':\n",
        "                components_l2 = find_section_by_path(sec_l1, ['component'], [])\n",
        "                # Убеждаемся, что components_l2 является списком\n",
        "                if not isinstance(components_l2, list): components_l2 = [components_l2]\n",
        "                for comp_l2 in components_l2:\n",
        "                    sec_l2 = comp_l2.get('section', {})\n",
        "                    if sec_l2 and find_section_by_path(sec_l2,['code','@code']) == 'STATEDIS':\n",
        "                        text_node = sec_l2.get('text')\n",
        "                        # Извлекаем текст из HTML-подобной структуры внутри <text>\n",
        "                        return extract_table_text_from_html_like(text_node)\n",
        "    # Если по коду не найдено, возвращаем результат поиска по display name (может быть пустой строкой)\n",
        "    return text_content"
      ],
      "metadata": {
        "id": "nUCq61pHXa6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_patient_records(patient_records_with_meta):\n",
        "    if not patient_records_with_meta:\n",
        "        return None\n",
        "\n",
        "    patient_records_data = [record['data'] for record in patient_records_with_meta]\n",
        "    first_visit_data = patient_records_data[0]\n",
        "\n",
        "    patient_id = get_patient_id(first_visit_data)\n",
        "    sex = get_patient_sex(first_visit_data)\n",
        "    birth_date_str = get_patient_birth_date_str(first_visit_data)\n",
        "    admission_date_str_first = get_admission_date_str(first_visit_data)\n",
        "    age = get_age_at_admission(birth_date_str, admission_date_str_first)\n",
        "\n",
        "    anamnesis_d = get_anamnesis_disease(first_visit_data)\n",
        "    anamnesis_l = get_anamnesis_life(first_visit_data)\n",
        "    condition_adm = get_condition_on_admission(first_visit_data)\n",
        "\n",
        "    diagnoses_adm_list_first_visit = get_all_diagnoses(first_visit_data, \"admission\")\n",
        "    diagnoses_dis_list_first_visit = get_all_diagnoses(first_visit_data, \"discharge\")\n",
        "\n",
        "    diagnoses_adm_texts = [f\"{d['mkb_code']}: {d['text']}\" for d in diagnoses_adm_list_first_visit]\n",
        "    diagnoses_dis_texts_fv = [f\"{d['mkb_code']}: {d['text']}\" for d in diagnoses_dis_list_first_visit]\n",
        "\n",
        "    instrumental_text = get_instrumental_studies_text(first_visit_data)\n",
        "    general_hosp_text = get_general_hospitalization_info_text(first_visit_data)\n",
        "    discharge_condition_text_fv = get_discharge_condition_text(first_visit_data)\n",
        "\n",
        "    text_features_parts = [\n",
        "        f\"Анамнез заболевания: {anamnesis_d}\" if anamnesis_d else \"\",\n",
        "        f\"Анамнез жизни: {anamnesis_l}\" if anamnesis_l else \"\",\n",
        "        f\"Состояние при поступлении: {condition_adm}\" if condition_adm else \"\",\n",
        "        f\"Диагнозы при поступлении: {'; '.join(diagnoses_adm_texts)}\" if diagnoses_adm_texts else \"\",\n",
        "        f\"Диагнозы при выписке (первый визит): {'; '.join(diagnoses_dis_texts_fv)}\" if diagnoses_dis_texts_fv else \"\",\n",
        "        f\"Результаты исследований (первый визит): {instrumental_text}\" if instrumental_text else \"\",\n",
        "        f\"Общие сведения о госпитализации (первый визит): {general_hosp_text}\" if general_hosp_text else \"\",\n",
        "        f\"Состояние при выписке (первый визит): {discharge_condition_text_fv}\" if discharge_condition_text_fv else \"\"\n",
        "    ]\n",
        "    aggregated_text = clean_text(\" \".join(filter(None, text_features_parts)))\n",
        "\n",
        "    # Собираем МКБ коды только со второго и последующих визитов\n",
        "    subsequent_visits_mkb_codes = set()\n",
        "    mace_target = None # Инициализируем таргет\n",
        "\n",
        "    if len(patient_records_data) > 1:\n",
        "        mace_target = 0 # По умолчанию таргет 0, если есть последующие визиты, но нет MACE/смерти\n",
        "        for subsequent_visit_idx in range(1, len(patient_records_data)):\n",
        "            subsequent_visit_data = patient_records_data[subsequent_visit_idx]\n",
        "            discharge_outcome = get_discharge_outcome(subsequent_visit_data)\n",
        "\n",
        "            if discharge_outcome and clean_text(discharge_outcome).lower() in DEATH_DISPLAY_NAMES:\n",
        "                mace_target = 1\n",
        "                break # Найден летальный исход в последующем визите, устанавливаем таргет 1 и выходим\n",
        "\n",
        "            subsequent_diagnoses_adm = get_all_diagnoses(subsequent_visit_data, \"admission\")\n",
        "            subsequent_diagnoses_dis = get_all_diagnoses(subsequent_visit_data, \"discharge\")\n",
        "\n",
        "            # Собираем МКБ коды при поступлении и выписке из последующих визитов\n",
        "            for diag in subsequent_diagnoses_adm + subsequent_diagnoses_dis:\n",
        "                 mkb_code = diag.get('mkb_code', '').strip().upper()\n",
        "                 if mkb_code:\n",
        "                     subsequent_visits_mkb_codes.add(mkb_code)\n",
        "\n",
        "            # Проверяем коды МКБ из последующих визитов на наличие MACE для установки таргета\n",
        "            for diag in subsequent_diagnoses_adm + subsequent_diagnoses_dis:\n",
        "                mkb_code = diag.get('mkb_code', '').strip().upper()\n",
        "                if mkb_code in MACE_ICD_CODES:\n",
        "                    mace_target = 1\n",
        "                    break # Найден MACE код во последующем визите, устанавливаем таргет и выходим\n",
        "\n",
        "            if mace_target == 1: # Если таргет стал 1 в этом визите, завершаем проверку последующих визитов\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        'patient_id': patient_id,\n",
        "        'text_features': aggregated_text,\n",
        "        'age_at_first_admission': age,\n",
        "        'sex': sex,\n",
        "        'mace_target': mace_target,\n",
        "        'unique_mkb_codes': list(subsequent_visits_mkb_codes) # Коды из 2+ визитов\n",
        "    }"
      ],
      "metadata": {
        "id": "4jFubd9cXfN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "QWweX0UpXkgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/My Drive/med_records_8500.zip'\n",
        "extract_path = '/medical_records'\n",
        "\n",
        "# Создаем папку для извлечения, если она не существует\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Файл {zip_path} успешно распакован в папку {extract_path}\")"
      ],
      "metadata": {
        "id": "MLswtna4YvAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xml_pattern = \"/medical_records/med_records_8500/*.xml\" # Пример для файлов, начинающихся с EMD_EPIC_DISCHARGE_\n",
        "\n",
        "# Если файлы в другой директории, укажите путь:\n",
        "# current_directory = os.getcwd() # Получаем текущую рабочую директорию ноутбука\n",
        "# xml_pattern = os.path.join(current_directory, \"ваша_папка_с_xml\", \"*.xml\")\n",
        "\n",
        "\n",
        "all_records_by_patient = {}\n",
        "xml_files = glob.glob(xml_pattern)\n",
        "\n",
        "if not xml_files:\n",
        "    print(f\"Не найдено XML файлов по паттерну: {xml_pattern}\")\n",
        "    # Можно остановить выполнение, если файлы не найдены, или продолжить с пустым списком\n",
        "    # raise FileNotFoundError(f\"Не найдено XML файлов по паттерну: {xml_pattern}\")\n",
        "else:\n",
        "    print(f\"Найдено {len(xml_files)} XML файлов. Начинаю обработку...\")\n",
        "\n",
        "    for file_path in xml_files:\n",
        "        doc_dict = parse_xml_file(file_path)\n",
        "        if not doc_dict:\n",
        "            continue\n",
        "\n",
        "        patient_id = get_patient_id(doc_dict)\n",
        "        admission_date_str = get_admission_date_str(doc_dict)\n",
        "\n",
        "        if not patient_id:\n",
        "            print(f\"Не удалось извлечь ID пациента из файла: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        admission_date = None\n",
        "        if admission_date_str:\n",
        "            try:\n",
        "                admission_date = datetime.strptime(admission_date_str[:8], \"%Y%m%d\")\n",
        "            except ValueError:\n",
        "                print(f\"Некорректный формат даты поступления '{admission_date_str}' в файле: {file_path}.\")\n",
        "                admission_date = datetime.max # Для некорректных дат ставим максимальную, чтобы они были в конце\n",
        "        else:\n",
        "             print(f\"Отсутствует дата поступления в файле: {file_path}.\")\n",
        "             admission_date = datetime.max\n",
        "\n",
        "\n",
        "        if patient_id not in all_records_by_patient:\n",
        "            all_records_by_patient[patient_id] = []\n",
        "\n",
        "        all_records_by_patient[patient_id].append({\n",
        "            'admission_date': admission_date,\n",
        "            'data': doc_dict,\n",
        "            'file_path': file_path\n",
        "        })\n",
        "\n",
        "    for patient_id in all_records_by_patient:\n",
        "        all_records_by_patient[patient_id].sort(key=lambda x: x['admission_date'])\n",
        "\n",
        "    print(f\"Сгруппировано {len(all_records_by_patient)} уникальных пациентов.\")"
      ],
      "metadata": {
        "id": "kaKFUZcjXkQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data_list = []\n",
        "processed_patients = 0\n",
        "total_patients = len(all_records_by_patient)\n",
        "\n",
        "if total_patients > 0:\n",
        "    print(f\"Начинаю детальную обработку {total_patients} пациентов...\")\n",
        "    for patient_id, records_with_meta in all_records_by_patient.items():\n",
        "        processed_info = process_patient_records(records_with_meta)\n",
        "        if processed_info:\n",
        "            # Добавляем количество визитов в processed_info\n",
        "            processed_info['num_visits'] = len(records_with_meta)\n",
        "            final_data_list.append(processed_info)\n",
        "        processed_patients +=1\n",
        "        if processed_patients % 100 == 0 or processed_patients == total_patients:\n",
        "            print(f\"Обработано пациентов: {processed_patients}/{total_patients}\")\n",
        "\n",
        "    df_results = pd.DataFrame(final_data_list)\n",
        "\n",
        "    print(\"\\nОбновление таргета MACE на основе кодов выписки из 2+ визитов...\")\n",
        "    initial_mace_1_count = df_results['mace_target'].value_counts().get(1.0, 0)\n",
        "\n",
        "    # Проходим по DataFrame и обновляем таргет\n",
        "    for index, row in df_results.iterrows():\n",
        "        # Обновляем только если текущий таргет не 1 (т.е. не было летального исхода)\n",
        "        # и у пациента было более одного визита\n",
        "        if row['mace_target'] != 1.0 and row['num_visits'] > 1 and row['unique_mkb_codes']:\n",
        "            # Проверяем наличие MACE кодов в unique_mkb_codes (коды выписки 2+ визитов)\n",
        "            if any(code in MACE_ICD_CODES for code in row['unique_mkb_codes']):\n",
        "                df_results.loc[index, 'mace_target'] = 1.0\n",
        "\n",
        "    updated_mace_1_count = df_results['mace_target'].value_counts().get(1.0, 0)\n",
        "    print(f\"Количество пациентов с mace_target=1.0 после обновления: {updated_mace_1_count} (До обновления: {initial_mace_1_count})\")\n",
        "\n",
        "\n",
        "    print(\"\\nИтоговая таблица (первые 5 строк):\")\n",
        "    display(df_results.head())\n",
        "    print(f\"\\nРазмер таблицы: {df_results.shape}\")\n",
        "\n",
        "    if not df_results.empty:\n",
        "        print(f\"\\nРаспределение MACE таргета:\\n{df_results['mace_target'].value_counts(dropna=False)}\")\n",
        "        print(f\"\\nРаспределение количества визитов:\\n{df_results['num_visits'].value_counts().sort_index()}\")\n",
        "    else:\n",
        "        print(\"\\nИтоговая таблица пуста после обработки.\")\n",
        "else:\n",
        "    print(\"Нет данных для обработки (список пациентов пуст).\")\n",
        "    df_results = pd.DataFrame()"
      ],
      "metadata": {
        "id": "tqpNKM_jXyDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5127b92"
      },
      "source": [
        "# Сбор всех уникальных МКБ кодов из DataFrame\n",
        "all_unique_mkb_codes_in_data = set()\n",
        "for index, row in df_results.iterrows():\n",
        "    if row['unique_mkb_codes']:\n",
        "        all_unique_mkb_codes_in_data.update(row['unique_mkb_codes'])\n",
        "\n",
        "print(f\"Общее количество уникальных МКБ кодов в данных: {len(all_unique_mkb_codes_in_data)}\")\n",
        "\n",
        "# Сравнение с MACE_ICD_CODES\n",
        "mace_codes_in_data = all_unique_mkb_codes_in_data.intersection(MACE_ICD_CODES)\n",
        "print(f\"Количество МКБ кодов из списка MACE, найденных в данных: {len(mace_codes_in_data)}\")\n",
        "\n",
        "# Вывод МКБ кодов из списка MACE, которые есть в данных\n",
        "if mace_codes_in_data:\n",
        "    print(\"\\nМКБ коды из списка MACE, найденные в данных:\")\n",
        "    print(sorted(list(mace_codes_in_data)))\n",
        "else:\n",
        "    print(\"\\nВ данных не найдено МКБ кодов, соответствующих списку MACE_ICD_CODES.\")\n",
        "\n",
        "# Вывод МКБ кодов в данных, которые не входят в список MACE_ICD_CODES (первые 20 для примера)\n",
        "other_codes_in_data = list(all_unique_mkb_codes_in_data - MACE_ICD_CODES)\n",
        "print(f\"\\nКоличество других МКБ кодов в данных (не из списка MACE): {len(other_codes_in_data)}\")\n",
        "if other_codes_in_data:\n",
        "    print(\"Примеры других МКБ кодов в данных (первые 20):\")\n",
        "    print(sorted(other_codes_in_data)[:20])\n",
        "\n",
        "# Анализ пациентов с mace_target=0 и NaN\n",
        "mace_0_patients = df_results[df_results['mace_target'] == 0.0]\n",
        "mace_nan_patients = df_results[df_results['mace_target'].isna()]\n",
        "\n",
        "print(f\"\\nКоличество пациентов с mace_target = 0.0: {len(mace_0_patients)}\")\n",
        "print(f\"Количество пациентов с mace_target = NaN: {len(mace_nan_patients)}\")\n",
        "\n",
        "# Проверка наличия MACE кодов у пациентов с mace_target = 0.0\n",
        "mace_0_with_mace_codes = mace_0_patients[mace_0_patients['unique_mkb_codes'].apply(lambda codes: any(code in MACE_ICD_CODES for code in codes))]\n",
        "print(f\"Количество пациентов с mace_target = 0.0, у которых есть МКБ коды из списка MACE: {len(mace_0_with_mace_codes)}\")\n",
        "\n",
        "# Проверка наличия MACE кодов у пациентов с mace_target = NaN\n",
        "mace_nan_with_mace_codes = mace_nan_patients[mace_nan_patients['unique_mkb_codes'].apply(lambda codes: any(code in MACE_ICD_CODES for code in codes))]\n",
        "print(f\"Количество пациентов с mace_target = NaN, у которых есть МКБ коды из списка MACE: {len(mace_nan_with_mace_codes)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Data"
      ],
      "metadata": {
        "id": "1HpBOJZwX01c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if not df_results.empty:\n",
        "#     output_filename = \"/content/drive/My Drive/parsed_medical_data.parquet\" # Указываем путь на Google Диск и формат Parquet\n",
        "#     df_results.to_parquet(output_filename, index=False) # Используем to_parquet\n",
        "#     print(f\"\\nТаблица сохранена в {output_filename}\")\n",
        "# else:\n",
        "#     print(\"\\nDataFrame пуст, сохранение не выполнено.\")"
      ],
      "metadata": {
        "id": "AUrPNOhrX0g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMS"
      ],
      "metadata": {
        "id": "9MliBRI1gH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download again"
      ],
      "metadata": {
        "id": "qq9BV88xgPeY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79593a04"
      },
      "source": [
        "# Загрузка таблицы из Google Диска\n",
        "output_filename = \"/content/drive/My Drive/parsed_medical_data.parquet\"\n",
        "try:\n",
        "    df_results = pd.read_parquet(output_filename)\n",
        "    print(f\"Таблица успешно загружена из {output_filename}\")\n",
        "    display(df_results.head())\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке таблицы из {output_filename}: {e}\")\n",
        "    df_results = pd.DataFrame() # Создаем пустой DataFrame в случае ошибки"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f9cc69"
      },
      "source": [
        "## Text data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0432588"
      },
      "source": [
        "import re\n",
        "\n",
        "def further_clean_text(text):\n",
        "    \"\"\"\n",
        "    Дополнительная очистка текста. Сохраняет заглавные буквы в начале предложений, аббревиатуры из 2-3 заглавных букв,\n",
        "    остальной текст приводит к нижнему регистру. Удаляет повторяющиеся символы и специфические паттерны.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Список служебных слов/местоимений, которые часто пишутся заглавными по ошибке\n",
        "    PRONOUNS_OR_FUNCTION_WORDS = {\"ЖЕ\", \"ТО\", \"ЛИ\", \"ИЛИ\", \"НЕ\", \"НИ\", \"БЫ\"}\n",
        "\n",
        "    # Предварительная очистка от специфических паттернов и лишних символов\n",
        "    text = text.replace('!!!!!!!!!!!!!', ' ')\n",
        "    text = text.replace('<.>', ' ')\n",
        "    # Заменяем группы небуквенных символов (кроме разрешенных) на пробелы\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s\\.,/\\-\\%()<>]+', ' ', text, flags=re.UNICODE)\n",
        "    # Заменяем множественные пробелы на один и удаляем пробелы по краям\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Обработка заглавных букв и аббревиатур\n",
        "    # Используем более надежное разбиение на предложения, сохраняя разделители для последующего объединения\n",
        "    sentence_parts = re.split(r'([.!?]\\s+)', text)\n",
        "    processed_sentences = []\n",
        "\n",
        "    current_sentence_words = []\n",
        "    is_first_word_of_sentence = True\n",
        "\n",
        "    for part in sentence_parts:\n",
        "        if not part:\n",
        "            continue\n",
        "        if re.fullmatch(r'[.!?]\\s+', part): # Если часть - это разделитель предложения\n",
        "            if current_sentence_words:\n",
        "                processed_sentences.append(\" \".join(current_sentence_words))\n",
        "            processed_sentences.append(part.strip()) # Добавляем разделитель\n",
        "            current_sentence_words = [] # Начинаем новое предложение\n",
        "            is_first_word_of_sentence = True\n",
        "        else: # Если часть - это текст предложения\n",
        "            words = part.split()\n",
        "            for word in words:\n",
        "                # Проверяем, является ли слово аббревиатурой (2 или 3 заглавные буквы)\n",
        "                if re.fullmatch(r'[А-ЯA-Z]{2,3}', word):\n",
        "                    current_sentence_words.append(word) # Сохраняем как аббревиатуру\n",
        "                    is_first_word_of_sentence = False\n",
        "                # Проверяем, является ли слово из списка служебных слов, написанных заглавными\n",
        "                elif word.upper() in PRONOUNS_OR_FUNCTION_WORDS and word.isupper():\n",
        "                     current_sentence_words.append(word.lower()) # Приводим к нижнему регистру\n",
        "                     is_first_word_of_sentence = False\n",
        "                elif is_first_word_of_sentence and word and word[0].isupper():\n",
        "                     # Сохраняем заглавную букву в начале первого слова предложения\n",
        "                     current_sentence_words.append(word[0] + word[1:].lower())\n",
        "                     is_first_word_of_sentence = False\n",
        "                else:\n",
        "                    current_sentence_words.append(word.lower()) # Приводим остальные слова к нижнему регистру\n",
        "    # Добавляем последнее предложение, если оно не было завершено знаком препинания\n",
        "    if current_sentence_words:\n",
        "         processed_sentences.append(\" \".join(current_sentence_words))\n",
        "\n",
        "    text = \" \".join(processed_sentences).strip() # Объединяем части обратно\n",
        "\n",
        "    # Удаление повторяющихся подряд слов (e.g., \"при при поступлении\")\n",
        "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
        "\n",
        "    # Ищем последовательность из слов, за которой следует та же последовательность\n",
        "    match_repeats = True\n",
        "    while match_repeats:\n",
        "        pattern = r'\\b((?:\\w+\\s+){1,}\\w+)\\s+\\1\\b' # Ищем повторение\n",
        "        new_text = re.sub(pattern, r'\\1', text, flags=re.IGNORECASE) # Игнорируем регистр при поиске повторов\n",
        "        if new_text == text:\n",
        "            match_repeats = False # Повторов больше нет\n",
        "        text = new_text\n",
        "\n",
        "\n",
        "\n",
        "    # финальная очистка\n",
        "    text = re.sub(r'(.)(?!\\1)(?<![\\.,])\\1+', r'\\1', text)\n",
        "\n",
        "    # Удаление множественных пробелов\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Применение очистки к столбцу text_features\n",
        "if not df_results.empty:\n",
        "    print(\"Применяю дополнительную очистку к столбцу 'text_features'...\")\n",
        "    df_results['text_features'] = df_results['text_features'].apply(further_clean_text)\n",
        "    print(\"Очистка завершена. Первые 5 строк с новым столбцом 'text_features':\")\n",
        "    display(df_results[['text_features']].head())\n",
        "else:\n",
        "    print(\"DataFrame пуст, дополнительная очистка не выполнена.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedGemma"
      ],
      "metadata": {
        "id": "6nFcGxtElIWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Доступно {torch.cuda.device_count()} GPU.\")\n",
        "    print(f\"Текущее GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU недоступно, используется CPU.\")\n",
        "\n",
        "# Для воспроизводимости\n",
        "SEED = 17"
      ],
      "metadata": {
        "id": "YUJ-hZki768H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Отбираем только данные с известными таргетами\n",
        "df_labeled = df_results[df_results['mace_target'].notna()].copy()\n",
        "df_labeled['mace_target'] = df_labeled['mace_target'].astype(int)\n",
        "print(f\"\\nРазмер датасета для обучения и теста: {len(df_labeled)}\")"
      ],
      "metadata": {
        "id": "0qIVP2jZlKLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"google/medgemma-4b-it\"\n",
        "\n",
        "# Квантизация для уменьшения использования памяти\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Для A100 используем bfloat16\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Загружаем базовую модель\n",
        "model = Gemma3ForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "MODEL_MAX_SEQ_LENGTH = 2048\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    model_max_length=MODEL_MAX_SEQ_LENGTH,\n",
        "    padding_side=\"right\",  # Явно задаем сторону паддинга, \"right\" - частый выбор\n",
        "    truncation_side=\"right\" # Явно задаем сторону обрезки\n",
        ")\n",
        "\n",
        "\n",
        "num_labels = 2\n",
        "model.config.num_labels = num_labels\n",
        "model.config.id2label = {i: f'LABEL_{i}' for i in range(num_labels)}\n",
        "model.config.label2id = {f'LABEL_{i}': i for i in range(num_labels)}\n",
        "\n",
        "\n",
        "# Добавляем классификационную голову поверх базовой модели\n",
        "classifier_input_dim = model.config.hidden_size\n",
        "\n",
        "\n",
        "# Настройка padding token если он не установлен\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "print(f\"\\nТокенизатор загружен. Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
        "print(f\"Базовая модель {MODEL_NAME} загружена с добавленной классификационной головой.\")\n",
        "print(f\"Конфигурация модели (первые несколько параметров):\")\n",
        "for k, v in list(model.config.to_dict().items()):\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "metadata": {
        "id": "BNNoBd178hY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим model_max_length токенизатора\n",
        "CONTEXT_WINDOW_SIZE = tokenizer.model_max_length\n",
        "print(f\"\\nМаксимальная длина последовательности для токенизатора (tokenizer.model_max_length): {CONTEXT_WINDOW_SIZE}\")\n",
        "# Устанавливаем PRACTICAL_MAX_LENGTH согласно рекомендациям для Med-Gemini 4b\n",
        "PRACTICAL_MAX_LENGTH = MODEL_MAX_SEQ_LENGTH\n",
        "print(f\"Практическая максимальная длина для токенизации: {PRACTICAL_MAX_LENGTH}\")"
      ],
      "metadata": {
        "id": "_hF7mToy_rls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_lengths = []\n",
        "for text in df_labeled['text_features']:\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    token_lengths.append(len(tokens))\n",
        "\n",
        "max_observed_len = np.max(token_lengths)\n",
        "avg_observed_len = np.mean(token_lengths)\n",
        "median_observed_len = np.median(token_lengths)\n",
        "percentile_95_len = np.percentile(token_lengths, 95)\n",
        "\n",
        "print(f\"Анализ длин токенизированных текстов (на {len(df_labeled)} примерах):\")\n",
        "print(f\"  Максимальная наблюдаемая длина: {max_observed_len} токенов\")\n",
        "print(f\"  Средняя наблюдаемая длина: {avg_observed_len:.2f} токенов\")\n",
        "print(f\"  Медианная наблюдаемая длина: {median_observed_len:.2f} токенов\")\n",
        "print(f\"  95-й перцентиль длины: {percentile_95_len:.2f} токенов\")\n",
        "\n",
        "if max_observed_len > PRACTICAL_MAX_LENGTH:\n",
        "    print(f\"\\n Максимальная наблюдаемая длина ({max_observed_len}) превышает выбранную практическую максимальную длину ({PRACTICAL_MAX_LENGTH}).\")\n",
        "    num_truncated = sum(1 for length in token_lengths if length > PRACTICAL_MAX_LENGTH)\n",
        "    print(f\"   Примерно {num_truncated} из {len(df_labeled)} ({num_truncated/len(df_labeled)*100:.2f}%) текстов будут проанализированны со скользящим окном.\")\n",
        "else:\n",
        "    print(f\"\\n Все тексты ({max_observed_len} токенов макс.) помещаются в выбранную практическую максимальную длину ({PRACTICAL_MAX_LENGTH}).\")"
      ],
      "metadata": {
        "id": "hG3Q6YkB8sXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df_labeled['text_features'],\n",
        "    df_labeled['mace_target'],\n",
        "    test_size=0.20, # 20% на тест\n",
        "    random_state=SEED,\n",
        "    stratify=df_labeled['mace_target']\n",
        ")\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_texts)}\")\n",
        "print(f\"Размер тестовой выборки: {len(test_texts)}\")\n",
        "print(f\"Распределение классов в обучающей выборке: {pd.Series(train_labels).value_counts(normalize=True)}\")\n",
        "print(f\"Распределение классов в тестовой выборке: {pd.Series(test_labels).value_counts(normalize=True)}\")\n",
        "\n",
        "# Создание словарей для Dataset\n",
        "train_data = {'text': list(train_texts), 'label': list(train_labels)}\n",
        "test_data = {'text': list(test_texts), 'label': list(test_labels)}\n",
        "\n",
        "# Преобразование в Hugging Face Dataset\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(\"\\nСтруктура датасета Hugging Face:\")\n",
        "print(raw_datasets)\n",
        "\n",
        "# Функция токенизации\n",
        "def tokenize_function(examples):\n",
        "    # Используем PRACTICAL_MAX_LENGTH, определенную ранее\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=PRACTICAL_MAX_LENGTH)\n",
        "    # padding=False здесь, т.к. DataCollatorWithPadding сделает это динамически для каждого батча\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data collator для динамического паддинга батчей до максимальной длины в батче\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "print(\"\\nПример токенизированного датасета:\")\n",
        "print(tokenized_datasets['train'][0])"
      ],
      "metadata": {
        "id": "Jwg442-69L3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (LoraConfig,\n",
        "                  PeftModel,\n",
        "                  prepare_model_for_kbit_training,\n",
        "                  get_peft_model,\n",
        "                  PeftModelForSequenceClassification,\n",
        "                  PeftConfig)\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput"
      ],
      "metadata": {
        "id": "cXcejlgH7__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM # Убедитесь, что Gemma3ForCausalLM импортирован\n",
        "from peft import PeftModelForSequenceClassification, PeftConfig\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n",
        "    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n",
        "        super().__init__(model, peft_config, adapter_name)\n",
        "        self.num_labels = 2\n",
        "        self.problem_type = \"single_label_classification\" # Используем \"single_label_classification\" для CrossEntropyLoss с метками [0, 1]\n",
        "\n",
        "        # Добавление классификационной головы к базовой модели, если ее нет\n",
        "        # self.base_model.model - это оригинальная Gemma3ForCausalLM\n",
        "        if not hasattr(self.base_model.model, 'score'):\n",
        "            classifier_input_dim = self.base_model.model.config.hidden_size\n",
        "            self.score_head = nn.Linear(classifier_input_dim, self.num_labels)\n",
        "\n",
        "            # Перемещаем новую голову на то же устройство, что и базовая модель\n",
        "            try:\n",
        "                # Пытаемся определить устройство модели более надежным способом\n",
        "                device = next(self.base_model.model.parameters()).device\n",
        "                self.score_head.to(device)\n",
        "            except StopIteration: # Если у модели нет параметров\n",
        "                # Фолбэк или логирование, если устройство не определено\n",
        "                print(\"Warning: Could not determine model device for score_head. Defaulting to cpu.\")\n",
        "                self.score_head.to('cpu')\n",
        "\n",
        "\n",
        "        elif hasattr(self.base_model.model, 'score') and isinstance(self.base_model.model.score, nn.Linear):\n",
        "            # Если score уже существует на базовой модели\n",
        "            # и это линейный слой, используем его.\n",
        "            self.score_head = self.base_model.model.score\n",
        "        else:\n",
        "            # Если score есть, но это не nn.Linear, или другая непредвиденная ситуация\n",
        "            raise TypeError(f\"Expected self.base_model.model.score to be nn.Linear or not exist, but found {type(self.base_model.model.score)}\")\n",
        "\n",
        "        # Убедимся, что конфигурация модели отражает количество меток\n",
        "        if self.config.num_labels != self.num_labels:\n",
        "            self.config.num_labels = self.num_labels\n",
        "        if self.config.problem_type != self.problem_type:\n",
        "            self.config.problem_type = self.problem_type\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,     # Параметр из сигнатуры, указывает, хочет ли Trainer/вызывающий вернуть attentions\n",
        "        output_hidden_states=None,  # Параметр из сигнатуры, указывает, хочет ли Trainer/вызывающий вернуть ВСЕ hidden_states\n",
        "        return_dict=None,\n",
        "        **kwargs):\n",
        "\n",
        "        # Определяем, нужно ли возвращать словарь, основываясь на аргументе или конфигурации\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        # Для внутреннего вызова базовой модели лучше всегда использовать return_dict=True для консистентности\n",
        "        effective_return_dict_for_base_call = True\n",
        "\n",
        "        # Вызов базовой PEFT-модели (которая оборачивает Gemma3ForCausalLM)\n",
        "        # Передаем запросы на output_attentions/output_hidden_states от вызывающей стороны\n",
        "        # к базовой модели. Таким образом, если Trainer их запрашивает, они будут вычислены.\n",
        "        # Мы всегда запрашиваем output_hidden_states=True для базовой модели,\n",
        "        # так как нам нужен последний скрытый слой для пулинга.\n",
        "        # Gradient checkpointing должен обрабатывать память для активаций промежуточных слоев базовой модели.\n",
        "        outputs_from_base = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True, # Нам нужен доступ к outputs_from_base.hidden_states\n",
        "            return_dict=effective_return_dict_for_base_call,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Извлекаем скрытые состояния последнего слоя.\n",
        "        # outputs_from_base.hidden_states - это кортеж всех скрытых состояний слоев.\n",
        "        last_layer_token_hidden_states = outputs_from_base.hidden_states[-1]\n",
        "\n",
        "        # Пулинг: выбираем скрытое состояние последнего значащего токена для каждой последовательности.\n",
        "        if attention_mask is not None:\n",
        "            sequence_lengths = torch.sum(attention_mask, dim=1) - 1\n",
        "            sequence_lengths = torch.clamp(sequence_lengths, min=0) # Для обработки пустых последовательностей\n",
        "\n",
        "            pooled_output = last_layer_token_hidden_states[\n",
        "                torch.arange(last_layer_token_hidden_states.size(0), device=last_layer_token_hidden_states.device),\n",
        "                sequence_lengths\n",
        "            ]\n",
        "        else:\n",
        "            # Если нет attention_mask, берем последний токен (менее надежно для паддинга)\n",
        "            pooled_output = last_layer_token_hidden_states[:, -1, :]\n",
        "\n",
        "        # Получаем логиты из нашей классификационной головы\n",
        "        # Если голова была добавлена как self.base_model.model.score:\n",
        "        # logits = self.base_model.model.score(pooled_output)\n",
        "        # Если голова определена как self.score_head в этом классе:\n",
        "        logits = self.score_head(pooled_output)\n",
        "\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            # Убедимся, что labels имеют тип LongTensor, как ожидает CrossEntropyLoss\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.long())\n",
        "\n",
        "        # Управляем возвращаемыми значениями в зависимости от return_dict,\n",
        "        if not return_dict: # Если вызывающий явно просил кортеж\n",
        "            output_items = (logits,)\n",
        "            # Включаем полные hidden_states или attentions только если они были запрошены\n",
        "            # через аргументы output_hidden_states/output_attentions этого метода forward\n",
        "            if output_hidden_states and hasattr(outputs_from_base, 'hidden_states') and outputs_from_base.hidden_states is not None:\n",
        "                output_items += (outputs_from_base.hidden_states,)\n",
        "            if output_attentions and hasattr(outputs_from_base, 'attentions') and outputs_from_base.attentions is not None:\n",
        "                output_items += (outputs_from_base.attentions,)\n",
        "\n",
        "            return ((loss,) + output_items) if loss is not None else output_items\n",
        "\n",
        "        # Если return_dict=True\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            # Возвращаем полные hidden_states/attentions только если вызывающий их просил.\n",
        "            # В противном случае возвращаем None, чтобы Trainer не накапливал их.\n",
        "            hidden_states=outputs_from_base.hidden_states if output_hidden_states else None,\n",
        "            attentions=outputs_from_base.attentions if output_attentions else None,\n",
        "        )"
      ],
      "metadata": {
        "id": "A6Hlo8uajUFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "peft_model = Gemma3ForSequenceClassification(lora_config, model)\n",
        "\n",
        "print(\"\\nМодель после применения LORA:\")\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "7Zk1vWY59YXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    try:\n",
        "        auc = roc_auc_score(labels, logits[:,1]) # Вероятности для положительного класса\n",
        "    except ValueError:\n",
        "        auc = 0.5\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": auc}"
      ],
      "metadata": {
        "id": "bTFzYjSu9i-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train only with targets"
      ],
      "metadata": {
        "id": "R-BeK9PnlNFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"./results_medgemma_lora_mace\"\n",
        "LOGGING_DIR = './logs_medgemma_lora_mace'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    label_names=[\"labels\"],\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=15,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", # Оценивать после каждой эпохи\n",
        "    save_strategy=\"epoch\",       # Сохранять модель после каждой эпохи\n",
        "    load_best_model_at_end=True, # Загрузить лучшую модель в конце обучения\n",
        "    metric_for_best_model=\"auc\",  # Метрика для определения лучшей модели\n",
        "    logging_dir=LOGGING_DIR,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=3,\n",
        "    report_to=\"tensorboard\",\n",
        "    fp16=False, # Для A100 используем bf16\n",
        "    bf16=True,  # Включаем bfloat16 для A100.\n",
        "    gradient_checkpointing=True, # Включаем градиентный чекпоинтинг для экономии памяти\n",
        "    max_grad_norm=0.3, # Добавляем клиппинг\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Создание Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "POpk_i_X8TYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zlUDYTuqCvR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62dfc9a1"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"Отображение Warnings отключено.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Начинаем обучение модели...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Сохранение метрик обучения\n",
        "train_metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", train_metrics)\n",
        "trainer.save_metrics(\"train\", train_metrics)\n",
        "\n",
        "print(\"\\nОбучение завершено.\")"
      ],
      "metadata": {
        "id": "B0Yd7RzI98EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nОценка лучшей модели на тестовой выборке:\")\n",
        "eval_metrics = trainer.evaluate()\n",
        "\n",
        "trainer.log_metrics(\"eval\", eval_metrics)\n",
        "trainer.save_metrics(\"eval\", eval_metrics)\n",
        "\n",
        "print(\"\\nМетрики на тестовой выборке:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "S2dz9NWL99-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADAPTER_OUTPUT_DIR = \"/content/drive/My Drive/medgemma_lora_mace_adapter\"\n",
        "peft_model.save_pretrained(ADAPTER_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_OUTPUT_DIR) # Сохраняем токенизатор вместе с адаптером для удобства\n",
        "\n",
        "print(f\"\\nОбученный LoRA адаптер и токенизатор сохранены в: {ADAPTER_OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "-runv5xZ9-b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fd7c52"
      },
      "source": [
        "### Inference on the full dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_id, test_id, train_labels, test_labels = train_test_split(\n",
        "    df_labeled['patient_id'],\n",
        "    df_labeled['mace_target'],\n",
        "    test_size=0.20, # 20% на тест\n",
        "    random_state=SEED,\n",
        "    stratify=df_labeled['mace_target']\n",
        ")\n",
        "\n",
        "train_data_labels = {'text': list(train_texts), 'label': list(train_labels), 'patient_id': train_id}\n",
        "test_data_labels = {'text': list(test_texts), 'label': list(test_labels), 'patient_id': test_id}"
      ],
      "metadata": {
        "id": "8KjvFKZ6FDIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
        "    trainer.model.eval()\n",
        "elif 'peft_model' in locals(): # Если trainer был удален, но peft_model осталась\n",
        "    peft_model.eval()\n",
        "else:\n",
        "    raise ValueError(\"Модель (trainer или peft_model) не найдена. Убедитесь, что обучение было завершено.\")\n",
        "\n",
        "# --- Предсказания для обучающей выборки ---\n",
        "print(\"\\nПолучение предсказаний для обучающей выборки...\")\n",
        "if \"train\" in tokenized_datasets:\n",
        "    train_pred_output = trainer.predict(tokenized_datasets[\"train\"])\n",
        "    train_logits = train_pred_output.predictions\n",
        "    # Преобразование логитов в вероятности для класса 1 (MACE)\n",
        "    train_probabilities_mace = torch.softmax(torch.tensor(train_logits), dim=-1)[:, 1].numpy()\n",
        "    # Исходные метки и ID пациентов из датасета\n",
        "    train_actual_labels = tokenized_datasets[\"train\"][\"label\"]\n",
        "    train_patient_ids_ds = train_data_labels[\"patient_id\"]\n",
        "\n",
        "    train_results_list = []\n",
        "    for i in range(len(train_patient_ids_ds)):\n",
        "        train_results_list.append({\n",
        "            'patient_id': train_patient_ids_ds.values[i],\n",
        "            'dataset_type': 'train',\n",
        "            'mace_probability_score': train_probabilities_mace[i],\n",
        "            'actual_mace_target': train_actual_labels[i]\n",
        "        })\n",
        "    print(f\"Обработано {len(train_results_list)} записей из обучающей выборки.\")\n",
        "else:\n",
        "    print(\"Обучающая выборка (tokenized_datasets['train']) не найдена.\")\n",
        "    train_results_list = []\n",
        "\n",
        "# --- Предсказания для тестовой выборки ---\n",
        "print(\"\\nПолучение предсказаний для тестовой выборки...\")\n",
        "if \"test\" in tokenized_datasets:\n",
        "    test_pred_output = trainer.predict(tokenized_datasets[\"test\"])\n",
        "    test_logits = test_pred_output.predictions\n",
        "    # Преобразование логитов в вероятности для класса 1 (MACE)\n",
        "    test_probabilities_mace = torch.softmax(torch.tensor(test_logits), dim=-1)[:, 1].numpy()\n",
        "    # Исходные метки и ID пациентов из датасета\n",
        "    test_actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
        "    test_patient_ids_ds = test_data_labels[\"patient_id\"]\n",
        "\n",
        "    test_results_list = []\n",
        "    for i in range(len(test_patient_ids_ds)):\n",
        "        test_results_list.append({\n",
        "            'patient_id': test_patient_ids_ds.values[i],\n",
        "            'dataset_type': 'test',\n",
        "            'mace_probability_score': test_probabilities_mace[i],\n",
        "            'actual_mace_target': test_actual_labels[i]\n",
        "        })\n",
        "    print(f\"Обработано {len(test_results_list)} записей из тестовой выборки.\")\n",
        "else:\n",
        "    print(\"Тестовая выборка (tokenized_datasets['test']) не найдена.\")\n",
        "    test_results_list = []\n",
        "\n",
        "# --- Объединение результатов в один DataFrame ---\n",
        "if train_results_list or test_results_list:\n",
        "    final_scores_df = pd.DataFrame(train_results_list + test_results_list)\n",
        "    print(\"\\nИтоговый DataFrame с предсказаниями (первые 5 строк):\")\n",
        "    display(final_scores_df.head())\n",
        "    print(f\"\\nРазмер итогового DataFrame: {final_scores_df.shape}\")\n",
        "\n",
        "    # --- Сохранение DataFrame на Google Диск ---\n",
        "    output_scores_filename = \"/content/drive/My Drive/gemma_mace_predictions.parquet\"\n",
        "    try:\n",
        "        final_scores_df.to_parquet(output_scores_filename, index=False)\n",
        "        print(f\"\\nDataFrame с предсказаниями успешно сохранен в: {output_scores_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nОшибка при сохранении DataFrame: {e}\")\n",
        "else:\n",
        "    print(\"\\nНет данных для создания итогового DataFrame.\")\n",
        "\n",
        "# Очистка памяти\n",
        "import gc\n",
        "del train_pred_output, test_pred_output, train_logits, test_logits\n",
        "del train_probabilities_mace, test_probabilities_mace\n",
        "del final_scores_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FXO4pDw2Edjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}